{
  "hash": "09f5aab2672bc8c576d2d31ded0abe64",
  "result": {
    "markdown": "---\ntitle: \"Classification\"\nauthor: \"Spencer Paragas\"\ndate: last-modified\ncategories: [cs5805final, code, analysis]\nformat: \n  html: \n    mermaid: \n      theme: default\n    code-tools: \n      source: \"https://github.com/skparagas/CS-5805-Final-Project/blob/master/blog/classification/class.qmd\"\n---\n\nThis is the fourth blog post in a series of posts for my class on machine learning at Virginia Tech. Here we will discuss classification, one of the most common techniques in machine learning that allows us to classify data. We'll cover logistic regression and decision trees, and we'll introduce the concept of training and testing.\n\n### The Data\n\nWe've already introduced the dataset we'll be using for this blog series in the previous blogs. In case you didn't read them, however, here's a quick summary.\n\nThe data was collected by Burn, the VT Men's Ultimate Club. Ultimate, also known simply as frisbee, is a game that involves players trying to throw a disc between themselves with the goal of being in their team's endzone with the disc, scoring them a point. Don't worry too much about the intricacies of ultimate; the machine learning concepts should still make sense regardless.\n\nThe data represents the performance of Burn's offense at a recent tournament. Each row represents an action with the disc, either throwing it or picking it up.\n\n::: {.cell execution_count=1}\n```` { .cell-code}\n```{{python}}\nimport numpy as np\nimport pandas as pd\ndat = pd.read_csv(\"currDatSteelCity23.csv\")\ndat = dat.drop(['Unnamed: 0'], axis=1)\ndat.head()\n```\n\n````\n\n::: {.cell-output .cell-output-display execution_count=1}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Opponent</th>\n      <th>idx</th>\n      <th>DJI.ID</th>\n      <th>Full.ID</th>\n      <th>Game.ID</th>\n      <th>Point.ID</th>\n      <th>Possession.ID</th>\n      <th>Defense.Type</th>\n      <th>Handler</th>\n      <th>Receiver</th>\n      <th>...</th>\n      <th>BasicEPAOpp</th>\n      <th>TrueEPAHan</th>\n      <th>TrueEPARec</th>\n      <th>EPA</th>\n      <th>OppFactor2</th>\n      <th>EPAHan</th>\n      <th>EPARec</th>\n      <th>EPAHanRes</th>\n      <th>EPAHanTot</th>\n      <th>EPAHan2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Binghamton</td>\n      <td>538</td>\n      <td>472</td>\n      <td>1.1.1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>Man</td>\n      <td>NaN</td>\n      <td>Micah</td>\n      <td>...</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Binghamton</td>\n      <td>539</td>\n      <td>472</td>\n      <td>1.1.1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>Man</td>\n      <td>Micah</td>\n      <td>John</td>\n      <td>...</td>\n      <td>0.024</td>\n      <td>0.018</td>\n      <td>0.005</td>\n      <td>0.053</td>\n      <td>0.0</td>\n      <td>0.039</td>\n      <td>0.014</td>\n      <td>0.015</td>\n      <td>0.054</td>\n      <td>0.059</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Binghamton</td>\n      <td>540</td>\n      <td>472</td>\n      <td>1.1.1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>Man</td>\n      <td>John</td>\n      <td>Dan B</td>\n      <td>...</td>\n      <td>0.011</td>\n      <td>0.008</td>\n      <td>0.002</td>\n      <td>0.042</td>\n      <td>0.0</td>\n      <td>0.031</td>\n      <td>0.011</td>\n      <td>0.000</td>\n      <td>0.031</td>\n      <td>0.046</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Binghamton</td>\n      <td>541</td>\n      <td>472</td>\n      <td>1.1.1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>Man</td>\n      <td>Dan B</td>\n      <td>Gribs</td>\n      <td>...</td>\n      <td>-0.249</td>\n      <td>-0.249</td>\n      <td>0.000</td>\n      <td>-0.420</td>\n      <td>0.0</td>\n      <td>-0.420</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>-0.420</td>\n      <td>-0.420</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Binghamton</td>\n      <td>542</td>\n      <td>472</td>\n      <td>1.1.2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>Man</td>\n      <td>NaN</td>\n      <td>Micah</td>\n      <td>...</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 40 columns</p>\n</div>\n```\n:::\n:::\n\n\n### classification\n\nClassification aims to classify or categorize data into predefined classes or categories. It is a type of supervised learning, meaning it requires labeled data for training (if you read my second blog on clustering, you'll recall that those were unsupervised learning algorithms). Labeled data consists of input features, or independent variables, and their corresponding class labels, or dependent variable. The goal is to build a model that can learn from labeled training data and then predict the class or category of unseen testing data.\n\nThere are two main parts to the classification process:\n\n1. Training: This involves using a set of labeled data called the train set to build the model. The model learns the patterns and relationships between the input features and the classes.\n2. Testing: Once the model is trained, it is evaluated using a separate set of labeled data called the test set. The performance of the model is measured using evaluation metrics like accuracy, precision, recall, and F1 scores.\n\nThese two phases are the backbone of a credible classification process (and though we didn't discuss it, they're just as important in regression too). We mentioned in the previous blog the concept of overfitting, or fitting a model so complex that it performs well on the data it's built on but not on unseen data. The use of testing data as an evaluation stage is one of the best ways to prevent overfitting.\n\nThere are other important parts of classification, like data wrangling and feature selection, but we'll forego discussing those for now.\n\n### Logistic Regression\n\nThere are many different types of classification models in machine learning. Here are some of the more commonly used ones:\n\n* Logistic Regression\n* Decision Trees\n* Random Forests\n* Support Vector Machines\n\nWe'll start by covering the simplest of all classification models: logistic regression. Although it's called a regression model, it's purpose is more closely aligned with classification models like decision trees and SVMs. While other regression models look at the relationship between the features and some response, linear regressions using a linear function and polynomial regressions using a polynomial function, logistic regressions look at the relationship between the features and the probability of the class label using a logistic function. The logistic function is a type of sigmoid functions. These function have an S-shaped curve which is bounded at the top and bottom. My professor, Dr. Laptev, provided [the code](https://maptv.github.io/blog/prob/) to a wonderful plot of what a logistic function looks like:\n\n::: {.cell execution_count=2}\n```` { .cell-code}\n```{{python}}\n# https://github.com/ageron/handson-ml3/blob/main/04_training_linear_models.ipynb\nimport matplotlib.pyplot as plt \nlim = 6\nt = np.linspace(-lim, lim, 100)\nsig = 1 / (1 + np.exp(-t))\n\nplt.figure(figsize=(8, 3))\nplt.plot([-lim, lim], [0, 0], \"k-\")\nplt.plot([-lim, lim], [0.5, 0.5], \"k:\")\nplt.plot([-lim, lim], [1, 1], \"k:\")\nplt.plot([0, 0], [-1.1, 1.1], \"k-\")\nplt.plot(t, sig, \"b-\", linewidth=2, label=r\"$\\sigma(t) = \\dfrac{1}{1 + e^{-t}}$\")\nplt.xlabel(\"t\")\nplt.legend(loc=\"upper left\")\nplt.axis([-lim, lim, -0.1, 1.1])\nplt.gca().set_yticks([0, 0.25, 0.5, 0.75, 1])\nplt.grid()\nplt.show()\n```\n\n````\n\n::: {.cell-output .cell-output-display}\n![](class_files/figure-html/cell-3-output-1.png){width=658 height=282}\n:::\n:::\n\n\nAs you can see, the function has an upper bound of 1 and a lower bound of 0. Though they can handle multi-class classification, logistic regressions are typically used for binary classification problems. The example we'll be using it on will follow the latter. We'll use the result of each throw in our data as class to be predicted. This is a binary variable where the output is either 0 (a turnover) or 1 (a completion). We'll use variables like the opponent, the defense type, the x- and y-coordinates on the field the throw came from and went to, etc. Let's try it out\n\n::: {.cell execution_count=3}\n```` { .cell-code}\n```{{python}}\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nfrom sklearn.metrics import ConfusionMatrixDisplay\n\nX = dat[['Opponent', 'Defense.Type', 'start.x', 'start.y', 'end.x', 'end.y', 'Force.Break', 'Throw.Type', 'Throw.Group']]\nenc = LabelEncoder()\nfor col in ['Opponent', 'Defense.Type', 'start.x', 'start.y', 'end.x', 'end.y', 'Force.Break', 'Throw.Type', 'Throw.Group']:\n    X[col] = X[col].astype('str')\n    X[col] = enc.fit_transform(X[col])\n\ny = dat['Action.Result']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nlog_reg = LogisticRegression()\nlog_reg.fit(X_train, y_train)\ny_test_pred = log_reg.predict(X_test)\nprint(\"Testing accuracy:\", accuracy_score(y_test, y_test_pred))\nmatrix = confusion_matrix(y_test, y_test_pred)\nprint(\"Testing accuracy:\", (matrix.diagonal()/matrix.sum(axis=1))[0])\nprint(\"Testing accuracy:\", (matrix.diagonal()/matrix.sum(axis=1))[1])\nprint(\"Testing Classification Report\")\nprint(classification_report(y_test, y_test_pred))\n\ncmd = ConfusionMatrixDisplay.from_estimator(log_reg, X_test, y_test)\nplt.show()\n```\n\n````\n\n::: {.cell-output .cell-output-stdout}\n```\nTesting accuracy: 0.8838383838383839\nTesting accuracy: 0.043478260869565216\nTesting accuracy: 0.9942857142857143\nTesting Classification Report\n              precision    recall  f1-score   support\n\n           0       0.50      0.04      0.08        23\n           1       0.89      0.99      0.94       175\n\n    accuracy                           0.88       198\n   macro avg       0.69      0.52      0.51       198\nweighted avg       0.84      0.88      0.84       198\n\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](class_files/figure-html/cell-4-output-2.png){width=504 height=429}\n:::\n:::\n\n\nThat didn't work well! It appears as though the data is too imbalanced; there are so many more data points where the throw was completed than the throw was turned over that the model struggles.\n\n### Decision Trees\n\nLet's try a decision tree instead. A decision tree is a flowchart-like structure where each internal node represents a feature or attribute, each branch represents a decision rule based on that feature, and each leaf node represents a class label.\n\nThe decision tree algorithm builds the tree recursively by partitioning the data based on different features and their values. The goal is to create a tree that can make accurate predictions by splitting the data in a way that maximizes the information gain or minimizes the impurity at each step.\n\n::: {.cell execution_count=4}\n```` { .cell-code}\n```{{python}}\n# Training the decision tree\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.tree import DecisionTreeClassifier\ntree_clf = DecisionTreeClassifier(criterion=\"entropy\", random_state=42, max_depth=40)\ntree_clf.fit(X_train, y_train)\n\n# Predicting on test data\ny_test_pred = tree_clf.predict(X_test)\nmatrix = confusion_matrix(y_test, y_test_pred)\nprint(\"Testing class accuracies:\", matrix.diagonal()/matrix.sum(axis=1))\nprint(\"Testing precision:\", precision_score(y_test, y_test_pred))\nprint(\"Testing recall:\", recall_score(y_test, y_test_pred))\nprint(\"Testing f1:\", f1_score(y_test, y_test_pred))\nprint(classification_report(y_test, y_test_pred))\n\ncmd = ConfusionMatrixDisplay.from_estimator(tree_clf, X_test, y_test)\nplt.show()\n```\n\n````\n\n::: {.cell-output .cell-output-stdout}\n```\nTesting class accuracies: [0.2173913  0.93714286]\nTesting precision: 0.9010989010989011\nTesting recall: 0.9371428571428572\nTesting f1: 0.9187675070028012\n              precision    recall  f1-score   support\n\n           0       0.31      0.22      0.26        23\n           1       0.90      0.94      0.92       175\n\n    accuracy                           0.85       198\n   macro avg       0.61      0.58      0.59       198\nweighted avg       0.83      0.85      0.84       198\n\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](class_files/figure-html/cell-5-output-2.png){width=504 height=429}\n:::\n:::\n\n\nThe decision tree worked even worse! It looks like this class label is a tough one to predict. Even so, these classification methods will take you for in predicting categories.\n\n\n### Conclusion\n\nHopefully you now understand the basics of classification. Be sure to check out the last installment of this series, and have a great day!\n\n",
    "supporting": [
      "class_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}