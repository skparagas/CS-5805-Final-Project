{
  "hash": "21fc949940fc1cefaa292600c81054a4",
  "result": {
    "markdown": "---\ntitle: \"Anomaly/Outlier Detection\"\nauthor: \"Spencer Paragas\"\ndate: last-modified\ncategories: [cs5805final, code, analysis]\nformat: \n  html: \n    mermaid: \n      theme: default\n    code-tools: \n      source: \"https://github.com/skparagas/CS-5805-Final-Project/blob/master/blog/outliers/outlier.qmd\"\n---\n\nThis is the fifth and final blog post in a series of posts for my class on machine learning at Virginia Tech. Here we will discuss outlier detection.\n\n### The Data\n\nWe've already introduced the dataset we'll be using for this blog series in the previous blogs. In case you didn't read them, however, here's a quick summary.\n\nThe data was collected by Burn, the VT Men's Ultimate Club. Ultimate, also known simply as frisbee, is a game that involves players trying to throw a disc between themselves with the goal of being in their team's endzone with the disc, scoring them a point. Don't worry too much about the intricacies of ultimate; the machine learning concepts should still make sense regardless.\n\nI mentioned in my first blog that we'll be using spinoffs of the initial dataset, which contained rows of throws from Burn's offense at a recent tournament. That's going to be the case in this blog. Burn has gone through the trouble of summarizing the data for each player on their team. We'll be using this dataset for our clustering purposes.\n\n::: {.cell execution_count=1}\n```` { .cell-code}\n```{{python}}\nimport numpy as np\nimport pandas as pd\ndat = pd.read_csv(\"playDatSteelCity23.csv\")\ndat = dat.drop(['Unnamed: 0'], axis=1)\ndat.head()\n```\n\n````\n\n::: {.cell-output .cell-output-display execution_count=14}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Player</th>\n      <th>ActivePoss</th>\n      <th>ActivePossScored</th>\n      <th>ScorePerc</th>\n      <th>Touches</th>\n      <th>PickUp</th>\n      <th>CenterThrow</th>\n      <th>DumpThrow</th>\n      <th>SwingThrow</th>\n      <th>UpLineThrow</th>\n      <th>...</th>\n      <th>RecEPA</th>\n      <th>HanEPARes</th>\n      <th>HanEPA2</th>\n      <th>HanEPATurn</th>\n      <th>RecEPATurn</th>\n      <th>EPA</th>\n      <th>TourneyScore</th>\n      <th>TourneyGrade</th>\n      <th>EPAPerPoss</th>\n      <th>EPAPerTouch</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>AR</td>\n      <td>20</td>\n      <td>7</td>\n      <td>35.0</td>\n      <td>54</td>\n      <td>6</td>\n      <td>0</td>\n      <td>0</td>\n      <td>9</td>\n      <td>2</td>\n      <td>...</td>\n      <td>0.398</td>\n      <td>0.718</td>\n      <td>1.108</td>\n      <td>-0.420</td>\n      <td>0.000</td>\n      <td>1.248</td>\n      <td>1.00</td>\n      <td>A</td>\n      <td>0.062377</td>\n      <td>0.023103</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>AK</td>\n      <td>10</td>\n      <td>5</td>\n      <td>50.0</td>\n      <td>25</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0.200</td>\n      <td>0.300</td>\n      <td>0.307</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.248</td>\n      <td>0.33</td>\n      <td>B</td>\n      <td>0.024833</td>\n      <td>0.009933</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>JL</td>\n      <td>41</td>\n      <td>15</td>\n      <td>36.6</td>\n      <td>115</td>\n      <td>2</td>\n      <td>0</td>\n      <td>3</td>\n      <td>21</td>\n      <td>6</td>\n      <td>...</td>\n      <td>1.843</td>\n      <td>0.956</td>\n      <td>-0.073</td>\n      <td>-2.074</td>\n      <td>-0.416</td>\n      <td>1.512</td>\n      <td>0.95</td>\n      <td>B</td>\n      <td>0.036869</td>\n      <td>0.013145</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ZA</td>\n      <td>16</td>\n      <td>7</td>\n      <td>43.8</td>\n      <td>40</td>\n      <td>4</td>\n      <td>1</td>\n      <td>1</td>\n      <td>5</td>\n      <td>3</td>\n      <td>...</td>\n      <td>-0.101</td>\n      <td>0.415</td>\n      <td>0.589</td>\n      <td>-0.829</td>\n      <td>-0.209</td>\n      <td>0.230</td>\n      <td>0.59</td>\n      <td>B</td>\n      <td>0.014361</td>\n      <td>0.005744</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>CM</td>\n      <td>29</td>\n      <td>10</td>\n      <td>34.5</td>\n      <td>119</td>\n      <td>19</td>\n      <td>2</td>\n      <td>5</td>\n      <td>12</td>\n      <td>5</td>\n      <td>...</td>\n      <td>0.229</td>\n      <td>0.637</td>\n      <td>0.612</td>\n      <td>-1.915</td>\n      <td>0.000</td>\n      <td>0.583</td>\n      <td>0.16</td>\n      <td>C</td>\n      <td>0.020089</td>\n      <td>0.004896</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 92 columns</p>\n</div>\n```\n:::\n:::\n\n\n### Outlier Detection\n\nOutlier analysis and anomaly detection are techniques used in data analysis to identify and understand unusual or abnormal observations in a dataset. These techniques help in detecting data points that deviate significantly from the expected patterns and behaviors.\n\nOutliers are data points that significantly different from the majority of the data. They can be caused by various factors such as measurement errors, data corruption, or rare events.\n\n### Boxplots\n\nBoxplots are great ways of noticing outliers. Let's try out a boxplot\n\n::: {.cell execution_count=2}\n```` { .cell-code}\n```{{python}}\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.boxplot(data=dat,x=dat[\"DumpThrow\"])\nplt.title(\"Boxplot of Number of Dump Reset Throws\")\nplt.show()\n```\n\n````\n\n::: {.cell-output .cell-output-display}\n![](outlier_files/figure-html/cell-3-output-1.png){width=545 height=449}\n:::\n:::\n\n\n### Isolation Forests\n\nIsolation Forest is an unsupervised learning algorithm that isolates outliers by randomly partitioning the data into subsets. It constructs an ensemble of decision trees (go to my classification blog to learn more about those) and identifies outliers as instances that require fewer partitions to be isolated. Outliers are expected to have shorter average path lengths in the tree structure compared to normal instances.\n\nTo test out this ML method, let's go back to our initial dataset.\n\n::: {.cell execution_count=3}\n```` { .cell-code}\n```{{python}}\nimport numpy as np\nimport pandas as pd\ndat = pd.read_csv(\"currDatSteelCity23.csv\")\ndat = dat.drop(['Unnamed: 0'], axis=1)\ndat.head()\n```\n\n````\n\n::: {.cell-output .cell-output-display execution_count=16}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Opponent</th>\n      <th>idx</th>\n      <th>DJI.ID</th>\n      <th>Full.ID</th>\n      <th>Game.ID</th>\n      <th>Point.ID</th>\n      <th>Possession.ID</th>\n      <th>Defense.Type</th>\n      <th>Handler</th>\n      <th>Receiver</th>\n      <th>...</th>\n      <th>BasicEPAOpp</th>\n      <th>TrueEPAHan</th>\n      <th>TrueEPARec</th>\n      <th>EPA</th>\n      <th>OppFactor2</th>\n      <th>EPAHan</th>\n      <th>EPARec</th>\n      <th>EPAHanRes</th>\n      <th>EPAHanTot</th>\n      <th>EPAHan2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Binghamton</td>\n      <td>538</td>\n      <td>472</td>\n      <td>1.1.1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>Man</td>\n      <td>NaN</td>\n      <td>Micah</td>\n      <td>...</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Binghamton</td>\n      <td>539</td>\n      <td>472</td>\n      <td>1.1.1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>Man</td>\n      <td>Micah</td>\n      <td>John</td>\n      <td>...</td>\n      <td>0.024</td>\n      <td>0.018</td>\n      <td>0.005</td>\n      <td>0.053</td>\n      <td>0.0</td>\n      <td>0.039</td>\n      <td>0.014</td>\n      <td>0.015</td>\n      <td>0.054</td>\n      <td>0.059</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Binghamton</td>\n      <td>540</td>\n      <td>472</td>\n      <td>1.1.1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>Man</td>\n      <td>John</td>\n      <td>Dan B</td>\n      <td>...</td>\n      <td>0.011</td>\n      <td>0.008</td>\n      <td>0.002</td>\n      <td>0.042</td>\n      <td>0.0</td>\n      <td>0.031</td>\n      <td>0.011</td>\n      <td>0.000</td>\n      <td>0.031</td>\n      <td>0.046</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Binghamton</td>\n      <td>541</td>\n      <td>472</td>\n      <td>1.1.1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>Man</td>\n      <td>Dan B</td>\n      <td>Gribs</td>\n      <td>...</td>\n      <td>-0.249</td>\n      <td>-0.249</td>\n      <td>0.000</td>\n      <td>-0.420</td>\n      <td>0.0</td>\n      <td>-0.420</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>-0.420</td>\n      <td>-0.420</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Binghamton</td>\n      <td>542</td>\n      <td>472</td>\n      <td>1.1.2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>Man</td>\n      <td>NaN</td>\n      <td>Micah</td>\n      <td>...</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 40 columns</p>\n</div>\n```\n:::\n:::\n\n\n::: {.cell execution_count=4}\n```` { .cell-code}\n```{{python}}\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import precision_score\n\nX = dat[['Opponent', 'Defense.Type', 'start.x', 'start.y', 'end.x', 'end.y', 'Force.Break', 'Throw.Type', 'Throw.Group']]\nenc = LabelEncoder()\nfor col in ['Opponent', 'Defense.Type', 'start.x', 'start.y', 'end.x', 'end.y', 'Force.Break', 'Throw.Type', 'Throw.Group']:\n    X[col] = X[col].astype('str')\n    X[col] = enc.fit_transform(X[col])\n\ny = dat['Action.Result']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nclf = IsolationForest(random_state=0)\nclf.fit(X_train)\ny_pred = clf.predict(X_test)\n\npred = pd.DataFrame({'pred': y_pred})\npred['y_pred'] = np.where(pred['pred'] == -1, 1, 0)\ny_pred = pred['y_pred'] \nprint(\"\")\nprint(\"\")\nprint(\"Precision:\", precision_score(y_test, y_pred))\n```\n\n````\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\nPrecision: 0.8503937007874016\n```\n:::\n:::\n\n\n### Conclusion\n\nHopefully you now understand the basics of clustering. Be sure to check out the next installment of this series, and have a great day!\n\n",
    "supporting": [
      "outlier_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}