{
  "hash": "18bfbe0d503642b6fbe52e9c424fdd78",
  "result": {
    "markdown": "---\ntitle: \"Clustering\"\nauthor: \"Spencer Paragas\"\ndate: last-modified\ncategories: [cs5805final, code, analysis]\nformat: \n  html: \n    mermaid: \n      theme: default\n    code-tools: \n      source: \"https://github.com/skparagas/CS-5805-Final-Project/blob/master/blog/clustering/cluster.qmd\"\n---\n\nThis is the second blog post in a series of posts for my class on machine learning at Virginia Tech. Here we will discuss clustering, a common technique in machine learning that involves the grouping of data points together. We'll cover the underlying objectives of clustering and dive into one of the most popular clustering methods: k-means.\n\n### The Data\n\nWe've already introduced the dataset we'll be using for this blog series in the previous blog. In case you didn't read that one, however, here's a quick summary.\n\nThe data was collected by Burn, the VT Men's Ultimate Club. Ultimate, also known simply as frisbee, is a game that involves players trying to throw a disc between themselves with the goal of being in their team's endzone with the disc, scoring them a point. Don't worry too much about the intricacies of ultimate; the machine learning concepts should still make sense regardless.\n\nI mentioned in my last blog that we'll be using spinoffs of the initial dataset, which contained rows of throws from Burn's offense at a recent tournament. That's going to be the case in this blog. Burn has gone through the trouble of summarizing the data for each player on their team. We'll be using this dataset for our clustering purposes.\n\n::: {.cell execution_count=2}\n```` { .cell-code}\n```{{python}}\nimport numpy as np\nimport pandas as pd\ndat = pd.read_csv(\"playDatSteelCity23.csv\")\ndat = dat.drop(['Unnamed: 0'], axis=1)\ndat.head()\n```\n\n````\n\n::: {.cell-output .cell-output-display execution_count=1}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Player</th>\n      <th>ActivePoss</th>\n      <th>ActivePossScored</th>\n      <th>ScorePerc</th>\n      <th>Touches</th>\n      <th>PickUp</th>\n      <th>CenterThrow</th>\n      <th>DumpThrow</th>\n      <th>SwingThrow</th>\n      <th>UpLineThrow</th>\n      <th>...</th>\n      <th>RecEPA</th>\n      <th>HanEPARes</th>\n      <th>HanEPA2</th>\n      <th>HanEPATurn</th>\n      <th>RecEPATurn</th>\n      <th>EPA</th>\n      <th>TourneyScore</th>\n      <th>TourneyGrade</th>\n      <th>EPAPerPoss</th>\n      <th>EPAPerTouch</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>AR</td>\n      <td>20</td>\n      <td>7</td>\n      <td>35.0</td>\n      <td>54</td>\n      <td>6</td>\n      <td>0</td>\n      <td>0</td>\n      <td>9</td>\n      <td>2</td>\n      <td>...</td>\n      <td>0.398</td>\n      <td>0.718</td>\n      <td>1.108</td>\n      <td>-0.420</td>\n      <td>0.000</td>\n      <td>1.248</td>\n      <td>1.00</td>\n      <td>A</td>\n      <td>0.062377</td>\n      <td>0.023103</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>AK</td>\n      <td>10</td>\n      <td>5</td>\n      <td>50.0</td>\n      <td>25</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0.200</td>\n      <td>0.300</td>\n      <td>0.307</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.248</td>\n      <td>0.33</td>\n      <td>B</td>\n      <td>0.024833</td>\n      <td>0.009933</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>JL</td>\n      <td>41</td>\n      <td>15</td>\n      <td>36.6</td>\n      <td>115</td>\n      <td>2</td>\n      <td>0</td>\n      <td>3</td>\n      <td>21</td>\n      <td>6</td>\n      <td>...</td>\n      <td>1.843</td>\n      <td>0.956</td>\n      <td>-0.073</td>\n      <td>-2.074</td>\n      <td>-0.416</td>\n      <td>1.512</td>\n      <td>0.95</td>\n      <td>B</td>\n      <td>0.036869</td>\n      <td>0.013145</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ZA</td>\n      <td>16</td>\n      <td>7</td>\n      <td>43.8</td>\n      <td>40</td>\n      <td>4</td>\n      <td>1</td>\n      <td>1</td>\n      <td>5</td>\n      <td>3</td>\n      <td>...</td>\n      <td>-0.101</td>\n      <td>0.415</td>\n      <td>0.589</td>\n      <td>-0.829</td>\n      <td>-0.209</td>\n      <td>0.230</td>\n      <td>0.59</td>\n      <td>B</td>\n      <td>0.014361</td>\n      <td>0.005744</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>CM</td>\n      <td>29</td>\n      <td>10</td>\n      <td>34.5</td>\n      <td>119</td>\n      <td>19</td>\n      <td>2</td>\n      <td>5</td>\n      <td>12</td>\n      <td>5</td>\n      <td>...</td>\n      <td>0.229</td>\n      <td>0.637</td>\n      <td>0.612</td>\n      <td>-1.915</td>\n      <td>0.000</td>\n      <td>0.583</td>\n      <td>0.16</td>\n      <td>C</td>\n      <td>0.020089</td>\n      <td>0.004896</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 92 columns</p>\n</div>\n```\n:::\n:::\n\n\n### Clustering\n\nClustering is a technique in machine learning that involves grouping similar data points together based on their inherent characteristics. It is an unsupervised learning method, meaning it does not require labeled data for training. This is because we aren't necessary trying to estimate or predict anything directly from clustering. Instead, we aim to discover patterns or structures within the data, enabling us to gain insights. Thus, we don't need our data to have labels, or context. \n\nLet's consider a simple example. Say we have a group of different animal species and we want to create subgroups of the animals based on similarity. Some animals, like lions and tigers, are extremely similar and will likely get grouped together. Other animals, like whales and mice, are not similar at all and will likely not get grouped together. From these groupings, we can analyze for patterns. We might notice that fish are all grouped together, indicating they're distinctly different type of animals then different animal classes. We might notice some exceptions, however, like dolphins (mammal) being grouped with sharks (fish). If this were a supervised learning method, we'd need information on the variables we're trying to estimate. If we were trying to estimate an animal's weight, for instance, we'd need data on animals and their weights to build the model in the first place. Since we're not trying to estimate anything, however, we don't absolutely need the animal's weight to make these groups, though an important feature like that will certainly help. \n\nWhile that example didn't entail math, clustering ML certainly does. Let's introduce the problem we'll be using for the rest of this blog. As I mentioned, the data we'll be using contains 26 rows of players on Burn. We'll be creating clusters of these players based on the number of times they pick up the disc (to start a possession) and their average throw distance. Let's plot the players using those two metrics:\n\n::: {.cell execution_count=3}\n```` { .cell-code}\n```{{python}}\nimport matplotlib.pyplot as plt\nx1 = dat['PickUp']\nx2 = dat['AvgThrow']\n\nfig = plt.figure(figsize = (6, 3))\nax = fig.add_subplot(111)\nplt.scatter(x1, x2, color = 'tan', s = 50, alpha = 0.5)\nplt.xlabel(\"Pick Ups\")\nplt.ylabel(\"Average Throw Distance\")\nplt.title(\"Pick Ups versus Throw Distance\")\nplt.show()\n```\n\n````\n\n::: {.cell-output .cell-output-display}\n![](cluster_files/figure-html/cell-3-output-1.png){width=510 height=302}\n:::\n:::\n\n\nJust from the eye test, you can likely see some clustering going on. The eye test is nice, but let's try out a more objective method.\n\n### $k$-Means\n\nOne commonly used clustering method is k-means clustering. In this algorithm, the data is divided into $k$ clusters, where $k$ is a parameter we define ourselves. The goal of the algorithm is to minimize the distance between data points and the center of the cluster they're assigned to (referred to as centroids). Here is the general structure for the algorithm:\n\n1. Initialize: select $k$ data points as the initial cluster centroids\n2. Assign: appoint each of our data points to the cluster whose centroid they're closest to\n3. Update: recalculate the cluster centroids by taking the mean of all data points assigned to each cluster\n4. Repeat: complete steps 2 and 3 until convergence\n\nThe algorithm is essentially entails updating the cluster of each data point and the centroid of each cluster until updating no longer change the results significantly or we get tired of it taking too long! These steps seem simple enough, but carrying it out will require further clarification. For instance, how do we decide on the value of $k$? How do we decide which centroid each data point is closest to? \n\nLet's start by answering the first question. Selecting $k$, the number of clusters, is a little messy. Having to choose that value at the very beginning of the algorithm is one of its largest downsides. The goal is to find the optimal $k$, one that minimizes the distance of data points within a cluster while maintaining the distance of between clusters. Choosing a too small $k$ can result in forcing data points that aren't close to be in the same cluster, while choosing a too large $k$ can result in little insights gained, as clusters become more individualized to each data point. Ideally, we'd have domain knowledge that may indicate the optimal number of clusters. If we don't, however, we'd likely have to turn to a selection technique, like the elbow method or silhouette analysis.\n\n### Elbow Method\n\nThe elbow method involves plotting the within-cluster sum of squares against different values of $k$. The within-cluster sum of squares essentially represents the distance within the cluster between their data points, where smaller values mean the data points in the same clusters are closer together. We'd expect this value to decrease as we increase $k$. After building this plot, we select the value where the decrease in the within-cluster sum of squares levels off. This is described as the \"elbow\" in the graph, as ideally the graph starts at a significant decline than suddenly flattens.\n\n::: {.cell execution_count=4}\n```` { .cell-code}\n```{{python}}\nfrom sklearn.cluster import KMeans\nX = np.array(list(zip(x1, x2))).reshape(len(x1), 2)\nK = range(1, 11)\nwcss = []\n\nfor k in K: \n    kmeans = KMeans(n_clusters = k, n_init = 10, random_state = 42)\n    kmeans.fit(X) \n    wcss.append(kmeans.inertia_)\n  \nplt.plot(K, wcss, 'bx-')\nplt.xlabel('$k$')\nplt.ylabel('Within-Cluster Sum of Squares')\nplt.title('Elbow Method')\nplt.show()\n```\n\n````\n\n::: {.cell-output .cell-output-display}\n![](cluster_files/figure-html/cell-4-output-1.png){width=602 height=449}\n:::\n:::\n\n\nThe elbow here is at $k=2$, so we'd choose 2 as our value for $k$ when running $k$-Means. That's just one method, however. Let's try another one.\n\n\n### Silhouette Analysis\n\nSilhouette analysis calculates a measure of how well each data point fits into its assigned cluster, helping to assess the quality of the clustering. It does this by calculating the average silhouette coefficient across all data points. The silhouette coefficient ranges from -1 to 1, where values close to 1 indicate the respective data point is in a suitable cluster and values close to -1 indicate the data point is in the wrong cluster. By calculating the average silhouette coefficient for different values of $k$, we can identify the value that maximizes the overall closeness within clusters and distance between clusters.\n\nFrom using the elbow method, we can tell that the optimal value for $k$ is likely 2. Let's use silhouette analysis to compare $k=2$ and $k=3$ to get some more confirmation.\n\n::: {.cell execution_count=5}\n```` { .cell-code}\n```{{python}}\nimport matplotlib.cm as cm\nfrom sklearn.metrics import silhouette_samples, silhouette_score\n\nK = range(2, 4)\n\nfor k in K:\n    fig = plt.figure(figsize = (6, 3))\n    ax = fig.add_subplot(111)\n    ax.set_xlim([-0.1, 1])\n    ax.set_ylim([0, len(X) + (k + 1) * 10])\n    kmeans = KMeans(n_clusters = k, n_init = 10, random_state = 42)\n    labels = kmeans.fit_predict(X)\n    silhouette_avg = silhouette_score(X, labels)\n    sample = silhouette_samples(X, labels)\n\n    y_lower = 10\n    for i in range(k):\n        cluster_values = sample[labels == i]\n        cluster_values.sort()\n        size = cluster_values.shape[0]\n        y_upper = y_lower + size\n        color = cm.nipy_spectral(float(i) / k)\n        ax.fill_betweenx(\n            np.arange(y_lower, y_upper),\n            0,\n            cluster_values,\n            facecolor=color,\n            edgecolor=color,\n            alpha=0.7,\n        )\n        ax.text(-0.05, y_lower + 0.5 * size, str(i))\n        y_lower = y_upper + 10\n\n    ax.set_title(\"Silhouette analysis ($k = %d$)\" % k)\n    ax.set_xlabel(\"Silhouette Coefficient\")\n    ax.set_ylabel(\"Cluster\")\n    ax.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n    ax.set_yticks([])\n    ax.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n\nplt.show()\n```\n\n````\n\n::: {.cell-output .cell-output-display}\n![](cluster_files/figure-html/cell-5-output-1.png){width=499 height=303}\n:::\n\n::: {.cell-output .cell-output-display}\n![](cluster_files/figure-html/cell-5-output-2.png){width=499 height=303}\n:::\n:::\n\n\nThe results of the silhouette analysis confirm what we found with the elbow method. We'll use $k=2$.\n\n### Results\n\nNow that we have our value for $k$, let's flesh out the rest of the $k$-Means algorithm. We'll randomly select our initial centroids. This is a limitation of $k$-Means, as we could converge to a locally optimal result. Without any domain knowledge to guide us, however, it's our easiest approach. We'll also use Euclidean distance to define how close each data point is to the centroids. This is one of the more commonly used distance metrics.\n\nWith that, we should be ready to run $k$-Means.\n\n::: {.cell execution_count=6}\n```` { .cell-code}\n```{{python}}\nk = 2\nfig = plt.figure(figsize = (6, 3))\nax = fig.add_subplot(111)\nkmeans = KMeans(n_clusters = k, n_init = 10, random_state = 42)\nkmeans.fit(X) \nlabels = kmeans.fit_predict(X)\ncolors = cm.nipy_spectral(labels.astype(float) / k)\nax.scatter(X[:, 0], X[:, 1], s = 50, alpha = 0.5, c = colors)\nax.set_xlabel(\"Pick Ups\")\nax.set_ylabel(\"Average Throw Distance\")\nax.set_title(\"Pick Ups versus Throw Distance\")\nplt.show()\n```\n\n````\n\n::: {.cell-output .cell-output-display}\n![](cluster_files/figure-html/cell-6-output-1.png){width=510 height=302}\n:::\n:::\n\n\nIt appears as though the separator between the two clusters is the number of times the players picked up the disc. In ultimate, there's a set role for the players that pick up the disc: the handler. The machine learning we did helped us see the number of handlers on the team and, when we check their names, who the handlers are. Great work!\n\n### Conclusion\n\nHopefully you now understand the basics of clustering. Be sure to check out the next installment of this series, and have a great day!\n\n",
    "supporting": [
      "cluster_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}