[
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Clustering",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\n# What are some clustering methods?"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About",
    "section": "",
    "text": "My name is Spencer Paragas. I am an associate analyst at Altria Client Services, as well as a graduate student in the Computer Science Masters of Engineering program at Virginia Tech. This blog is currently set up as the final project in my graduate class on machine learning. My hope, however, is that this blog will eventually turn into something beyond just this class."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "CS 5805 Final Project",
    "section": "",
    "text": "Probability Theory and Random Variables\n\n\n\n\n\n\n\ncs5805final\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 30, 2023\n\n\nSpencer Paragas\n\n\n\n\n\n\n  \n\n\n\n\nClustering\n\n\n\n\n\n\n\ncs5805final\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 30, 2023\n\n\nSpencer Paragas\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Probability Theory and Random Variables",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2\n\n\n\n# What is probability theory?\nprint(\"Hello World\")\n\n[1] \"Hello World\""
  },
  {
    "objectID": "blog/welcome/index.html",
    "href": "blog/welcome/index.html",
    "title": "Clustering",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\n\nCode\n```{python}\n# What are some clustering methods?\n2+2\n```\n\n\n4"
  },
  {
    "objectID": "blog/post-with-code/index.html",
    "href": "blog/post-with-code/index.html",
    "title": "Probability Theory and Random Variables",
    "section": "",
    "text": "This is a post with executable code.\n\nCode```{r}\n1 + 1\n```\n\n[1] 2\n\n\n\nCode```{r}\n# What is probability theory?\nprint(\"Hello World\")\n```\n\n[1] \"Hello World\""
  },
  {
    "objectID": "blog/probability theory/index.html",
    "href": "blog/probability theory/index.html",
    "title": "Probability Theory and Random Variables",
    "section": "",
    "text": "This is a post with executable code.\n\nCode```{r}\n1 + 1\n```\n\n[1] 2\n\n\n\nCode```{r}\n# What is probability theory?\nprint(\"Hello World\")\n```\n\n[1] \"Hello World\""
  },
  {
    "objectID": "blog/welcome/cluster.html",
    "href": "blog/welcome/cluster.html",
    "title": "Clustering",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\n\nCode\n```{python}\n# What are some clustering methods?\n2+2\n```\n\n\n4"
  },
  {
    "objectID": "blog/probability/index.html",
    "href": "blog/probability/index.html",
    "title": "Probability Theory and Random Variables",
    "section": "",
    "text": "This is the first blog post in a series of posts for my class on machine learning at Virginia Tech. Here we will discuss probability theory, which is the foundation to the later machine learning posts in this series. While probability theory is a massive branch of mathematics that can’t be fully covered in one blog, we’ll go over topics like random variables, conditional probability, and independence.\n\nThe Data\nBefore that, however, let’s introduce the dataset we’ll be using for the entirety of this blog series. The data was collected by Burn, the VT Men’s Ultimate Club. Ultimate, also known simply as frisbee, is a game that involves players trying to throw a disc between themselves with the goal of being in their team’s endzone with the disc, scoring them a point. For many of you, this game may be foreign, making some of the variables confusing. While you don’t need to know most of the context to understand the machine learning concepts shown, I’ll do my best to explain the important parts as succinctly as possible.\nThe data represents the performance of the ultimate club at Steel City, a tournament in Pittsburgh they went to this Fall. Only data on Burn’s offense (when they had the disc) was collected. In the initial dataset, each row represents an action with the disc, either throwing it or picking it up. The features include variables like the opponents they were facing, who threw it to who, the type of throw it was, and the outcome of the throw. In total the dataset is \\(986 \\times 40\\), though spinoffs of the dataset will be used later on in the series.\n\n\nCode\n```{python}\nimport numpy as np\nimport pandas as pd\ndat = pd.read_csv(\"currDatSteelCity23.csv\")\ndat = dat.drop(['Unnamed: 0'], axis=1)\ndat.head()\n```\n\n\n\n\n\n\n\n\n\nOpponent\nidx\nDJI.ID\nFull.ID\nGame.ID\nPoint.ID\nPossession.ID\nDefense.Type\nHandler\nReceiver\n...\nBasicEPAOpp\nTrueEPAHan\nTrueEPARec\nEPA\nOppFactor2\nEPAHan\nEPARec\nEPAHanRes\nEPAHanTot\nEPAHan2\n\n\n\n\n0\nBinghamton\n538\n472\n1.1.1\n1\n1\n1\nMan\nNaN\nMicah\n...\n0.000\n0.000\n0.000\n0.000\n0.0\nNaN\nNaN\n0.000\nNaN\nNaN\n\n\n1\nBinghamton\n539\n472\n1.1.1\n1\n1\n1\nMan\nMicah\nJohn\n...\n0.024\n0.018\n0.005\n0.053\n0.0\n0.039\n0.014\n0.015\n0.054\n0.059\n\n\n2\nBinghamton\n540\n472\n1.1.1\n1\n1\n1\nMan\nJohn\nDan B\n...\n0.011\n0.008\n0.002\n0.042\n0.0\n0.031\n0.011\n0.000\n0.031\n0.046\n\n\n3\nBinghamton\n541\n472\n1.1.1\n1\n1\n1\nMan\nDan B\nGribs\n...\n-0.249\n-0.249\n0.000\n-0.420\n0.0\n-0.420\n0.000\n0.000\n-0.420\n-0.420\n\n\n4\nBinghamton\n542\n472\n1.1.2\n1\n1\n2\nMan\nNaN\nMicah\n...\n0.000\n0.000\n0.000\n0.000\n0.0\nNaN\n0.000\n0.000\nNaN\nNaN\n\n\n\n\n5 rows × 40 columns\n\n\n\n\n\nRandom Variables\nA random variable (RV) is a variable of unknown value that represents the numerical outcome of some process. This is vague, but essentially an RV can’t be known (hence it’s a variable), has to be numerical, and has to have some kind of context to it. RVs are typically represented by capitalized letters, like \\(X\\) or \\(Y\\). Some examples include the number of coins that flip to head out of 2 or the height of a person. The former example is one where the RV is discrete: it can only be a distinct value like 0, 1, or 2. The latter example is one where the RV is continuous: it can be any of the infinite values within a range, like any number between 0 and 1000.\nThe significance of random variables in the world of probability theory and, more broadly, statistics in general is that RVs have probability distributions that allow us to calculate the probabilities of RVs. This process has many applications, where people try to understand and estimate real world RVs.\n\n\nProbability Distributions\nLet’s look at an example in the context of our ultimate data. Say we know that on any given throw, John has a 50% chance of completing the throw without turning it over to the other team. Our random variable \\(X_{1}\\) is the number of completed throws John makes off of 1 throw. This is RV is discrete. What would the resulting probability distribution look like? This one’s pretty simple, he has a 50% chance of completing the one throw, and he has a 50% chance of not completing it. Thus, the probability distribution looks like this:\n\n\nCode\n```{python}\nimport matplotlib.pyplot as plt\nx = {'0':0.5, '1':0.5}\ncompletions = list(x.keys())\nprobabilities = list(x.values())\n\nfig = plt.figure(figsize = (6, 3))\nplt.bar(completions, probabilities, color = 'cornflowerblue', width = 0.4)\nplt.xlabel(\"Number of Completed Throws\")\nplt.ylabel(\"Probability\")\nplt.title(\"Probability Distribution for $X_1$\")\nplt.show()\n```\n\n\n\n\n\nWhat about the probability distribution of the random variable \\(X_{2}\\), the number of completed throws John makes off of 2 throws? Each throw has an equal chance of being a completion or an incompletion (50%). Half of the time John will complete the first throw, and half of those times John will complete the second throw. That gives the probability of John completing 2 out of 2 throws 0.25. The same logic can give you the probability of 0 completions. Thus, the probability distribution for \\(X_{2}\\) looks like this:\n\n\nCode\n```{python}\nx = {'0':0.25, '1':0.5, '2':0.25}\ncompletions = list(x.keys())\nprobabilities = list(x.values())\n\nfig = plt.figure(figsize = (6, 3))\nplt.bar(completions, probabilities, color = 'cornflowerblue', width = 0.4)\nplt.xlabel(\"Number of Completed Throws\")\nplt.ylabel(\"Probability\")\nplt.title(\"Probability Distribution for $X_2$\")\nplt.show()\n```\n\n\n\n\n\n\\(X_{1}\\) and \\(X_{2}\\) are examples of random variables with a binomial distribution. This distribution represent RVs that are the number of successes in a set of trials, where each trial results in one of two outcomes: success or failure. We won’t go too much into detail on the math behind it, but here’s the formula for a binomial distribution:\n\\[\nP(k)=\\frac{n!}{k!(n-k!)}p^{k}(1-p)^{n-k}\n\\]\nwhere \\(k\\) is the number of successes, \\(n\\) is the number of trials, and \\(p\\) is the probability of success. \\(P(k)\\) is the probability of getting \\(k\\) successes.\n\n\nEvents\nSo now we know the probability distribution of John’s completions out of a set number of throws. If we had John actually go through with this experiment and we kept track of his throws and whether they were completed or not, we should get actual values to \\(X_{1}\\) and \\(X_{2}\\). When we collect a value for \\(X_{1}\\) one time, we’ll get either a completion or a turnover. This is known as an event and will be part of our sample (though right now it’s the only event in out sample). Graphing that, it won’t look like the probability distribution we laid out for \\(X_{1}\\). However, if we have John attempt this a large number of times, our results should resemble the probability distribution we assigned it. This describes the notion that the results of a sequence of RVs will converge towards their expectation as we gather more and more events in our sample, otherwise known as the law of large numbers. This theorem is one of the main principles of probability theory. Let’s test it out and run 10,000 events, or trials, for \\(X_{1}\\):\n\n\nCode\n```{python}\nn, p = 1, 0.5\ns = np.random.binomial(n, p, 10000)\ndf = pd.Series(s).value_counts() / len(s)\ndf = df.sort_index()\n\ncompletions = list(map(str, list(df.keys())))\nfreq = list(df.values)\n\nprint(df)\n\nfig = plt.figure(figsize = (6, 3))\nplt.bar(completions, freq, color = 'salmon', width = 0.4)\nplt.xlabel(\"Number of Completed Throws\")\nplt.ylabel(\"Relative Frequency\")\nplt.title(\"10,000 Events for $X_1$\")\nplt.show()\n```\n\n\n0    0.5041\n1    0.4959\ndtype: float64\n\n\n\n\n\nThat looks pretty close to me. Let’s test out \\(X_{2}\\) now:\n\n\nCode\n```{python}\nn, p = 2, 0.5\ns = np.random.binomial(n, p, 10000)\ndf = pd.Series(s).value_counts() / len(s)\ndf = df.sort_index()\n\ncompletions = list(map(str, list(df.keys())))\nfreq = list(df.values)\n\nprint(df)\n\nfig = plt.figure(figsize = (6, 3))\nplt.bar(completions, freq, color = 'salmon', width = 0.4)\nplt.xlabel(\"Number of Completed Throws\")\nplt.ylabel(\"Relative Frequency\")\nplt.title(\"10,000 Events for $X_2$\")\nplt.show()\n```\n\n\n0    0.2444\n1    0.5023\n2    0.2533\ndtype: float64\n\n\n\n\n\nLooks like it worked! Now that we know a little bit about random variables and their underlying probability distributions, we can further delve into the world of probabilities.\n\n\nProbability Theory\nProbability theory is, simply put, the math behind probabilities. We’ve already worked with the three main parts of probability theory:\n\nrandom variables\nprobability distributions\nevents\n\nBecause of this, we arguably just covered probability theory, but let’s go through some of the basics of how probabilities work just for good measure.\nProbability Principles:\n\n\\(P(x) \\in [0,1]\\)\n\\(P(\\Omega) = 1\\)\n\nLet’s start off with how probabilities work. The probability of \\(x\\), denoted by \\(P(x)\\), is the chance that \\(x\\) occurs. It must be a number between and including 0 and 1. A probability of 0 means the outcome will never occur, while a probability of 1 means the outcome will always occur. \\(\\Omega\\) denotes our sample space, or all the possible outcomes of the given event. The probability of one of these outcomes occurring is 1, as no other outcome can occur (otherwise it would be in the sample space).\nThe above was pretty intuitive, but it’s important to understand those concepts well. Now, let’s go over some useful definitions.\nProbability Definitions:\n\nIndependence: \\(P(A|B) = P(A)\\) or \\(P(B|A) = P(B)\\)\nMutually exclusive: \\(P(A\\cap B) = 0\\)\n\nTwo events are independent if the occurrence of one event does not change the probability of the other event. \\(P(A|B)\\) stands for the probability of \\(A\\) occurring given that \\(B\\) occurs. If that is equal to the probability of \\(A\\), then we can conclude that occurrence of \\(B\\) is independent of the occurrence of \\(A\\). The second definition describes two events that can’t both occur. One easy example is John throwing a completion and an incompletion. He can throw either one, but he can’t throw both at the same time. \\(P(A\\cap B)\\) stands for the probability of \\(A\\) and \\(B\\) both occurring. This is known as the intersection of \\(A\\) and \\(B\\). When that is equal to 0, \\(A\\) and \\(B\\) will never both occur, thus making them mutually exclusive. Another term for this is disjoint.\nThe above rules will be handy in the future, and we learned some important denotations along the way. Here’s two more: \\(P(A\\cup B\\) stands for the probability of \\(A\\) and/or \\(B\\) occurring. This is known as the union of \\(A\\) and \\(B\\). And \\(P(A')\\) stands for the probability of \\(A\\) not occurring. This is known as the complement of \\(A\\). Keeping these mind, let’s try some more complicated rules.\n\nAddition Rule: \\(P(A\\cup B) = P(A) + P(B) - P(A\\cap B)\\)\nMultiplication Rule: \\(P(A\\cup B) = P(A)\\times P(B|A) = P(B)\\times P(A|B)\\)\nComplement Rule: \\(P(A') = 1 - P(A)\\)\n\nThe addition rule describes how the union of two events works. Subtracting the intersection of both events is the key. It’s necessary because by adding the individual probabilities of the two events, you are inadvertently adding the intersection twice. Note that for mutually exclusive events \\(A\\) and \\(B\\), \\(P(A\\cap B) = 0\\), making \\(P(A\\cup B) = P(A) + P(B)\\). The multiplication rule further describes how union works. With this rule, if \\(A\\) and \\(B\\) are independent, then \\(P(A|B) = P(A)\\) and \\(P(B|A) = P(B)\\), making \\(P(A\\cup B) = P(A) * P(B)\\). The final rule describes how the complement of an event works. This can be reordered to show \\(P(A) + P(A') = 1\\), which makes intuitive sense. The probability of an event occurring plus the probability of that event not occurring should equal 1. The event has to either occur or not occur.\n\n\nMachine Learning Applications\nProbability theory is at the foundation of machine learning. ML algorithms typically make assumptions on the data using probability theory. These algorithms use those assumptions to make estimates and predictions on whatever unknown variable (like a random variable!) we’re focused on.\nLet’s go through an example. Say we are trying to estimate \\(Y\\), the number of throws that occur in a given possession for Burn. We want to know if \\(Z\\), the possession result (either a score or a turnover), is independent of \\(Y\\). First, let’s graph a distribution plot.\n\n\nCode\n```{python}\nimport seaborn as sns\npossession = dat.groupby('Full.ID') \\\n                .agg(Throws=('Possession.Result', 'size'), Score=('Possession.Result', 'mean')) \\\n                .reset_index()\nsns.displot(possession, x=\"Throws\", kde=True, hue=\"Score\", stat=\"count\")\nplt.show()\n```\n\n\n\n\n\nThe probability distribution of the number of throws appears to change significantly based on if the possession resulted in a score or not. Let’s try estimating this phenomenon with a ML algorithm.\n\n\nCode\n```{python}\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error                # metric for evaluating regression model\nfrom sklearn.metrics import mean_absolute_error               # metric for evaluating regression model\nfrom sklearn.metrics import r2_score                          # metric for evaluating regression model\n\nX = possession['Score']\nX = X.array.reshape(-1, 1)\ny = possession['Throws']\nlin_reg = LinearRegression().fit(X, y)\n\ny_pred = lin_reg.predict(X)\nprint(\"Linear Regression Results:\")\nprint(\"MAE:\", mean_absolute_error(y, y_pred))\nprint(\"MSE:\", mean_squared_error(y, y_pred))\nprint(\"r2:\", r2_score(y, y_pred))\n```\n\n\nLinear Regression Results:\nMAE: 3.2052691321520363\nMSE: 16.764008697346565\nr2: 0.053219132145364445\n\n\nWe just used the basics of probability theory to build a machine learning model. As you can tell by its low score (just a \\(r^{2}\\) value of 0.05), this model doesn’t run that well. Maybe that means that \\(Y\\) and \\(Z\\) are actually independent of each other (\\(P(Y|Z) = P(Y)\\)) and the differences we saw were just random error. Maybe the effect \\(Z\\) has on \\(Y\\) is just very slight.\n\n\nConclusion\nHopefully you now understand the basics of probability theory, the foundation for wondrous world of machine learning. Be sure to check out the next installment of this series, and have a great day!"
  },
  {
    "objectID": "blog/clustering/index.html",
    "href": "blog/clustering/index.html",
    "title": "Clustering",
    "section": "",
    "text": "This is the second blog post in a series of posts for my class on machine learning at Virginia Tech. Here we will discuss clustering, a common technique in machine learning that involves the grouping of data points together. We’ll cover the underlying objectives of clustering and dive into one of the most popular clustering methods: k-means.\n\nThe Data\nWe’ve already introduced the dataset we’ll be using for this blog series in the previous blog. In case you didn’t read that one, however, here’s a quick summary.\nThe data was collected by Burn, the VT Men’s Ultimate Club. Ultimate, also known simply as frisbee, is a game that involves players trying to throw a disc between themselves with the goal of being in their team’s endzone with the disc, scoring them a point. Don’t worry too much about the intricacies of ultimate; the machine learning concepts should still make sense regardless.\nI mentioned in my last blog that we’ll be using spinoffs of the initial dataset, which contained rows of throws from Burn’s offense at a recent tournament. That’s going to be the case in this blog. Burn has gone through the trouble of summarizing the data for each player on their team. We’ll be using this dataset for our clustering purposes.\n\n\nCode\n```{python}\nimport numpy as np\nimport pandas as pd\ndat = pd.read_csv(\"playDatSteelCity23.csv\")\ndat = dat.drop(['Unnamed: 0'], axis=1)\ndat.head()\n```\n\n\n\n\n\n\n\n\n\nPlayer\nActivePoss\nActivePossScored\nScorePerc\nTouches\nPickUp\nCenterThrow\nDumpThrow\nSwingThrow\nUpLineThrow\n...\nRecEPA\nHanEPARes\nHanEPA2\nHanEPATurn\nRecEPATurn\nEPA\nTourneyScore\nTourneyGrade\nEPAPerPoss\nEPAPerTouch\n\n\n\n\n0\nAR\n20\n7\n35.0\n54\n6\n0\n0\n9\n2\n...\n0.398\n0.718\n1.108\n-0.420\n0.000\n1.248\n1.00\nA\n0.062377\n0.023103\n\n\n1\nAK\n10\n5\n50.0\n25\n0\n0\n0\n2\n0\n...\n0.200\n0.300\n0.307\n0.000\n0.000\n0.248\n0.33\nB\n0.024833\n0.009933\n\n\n2\nJL\n41\n15\n36.6\n115\n2\n0\n3\n21\n6\n...\n1.843\n0.956\n-0.073\n-2.074\n-0.416\n1.512\n0.95\nB\n0.036869\n0.013145\n\n\n3\nZA\n16\n7\n43.8\n40\n4\n1\n1\n5\n3\n...\n-0.101\n0.415\n0.589\n-0.829\n-0.209\n0.230\n0.59\nB\n0.014361\n0.005744\n\n\n4\nCM\n29\n10\n34.5\n119\n19\n2\n5\n12\n5\n...\n0.229\n0.637\n0.612\n-1.915\n0.000\n0.583\n0.16\nC\n0.020089\n0.004896\n\n\n\n\n5 rows × 92 columns\n\n\n\n\n\nClustering\nClustering is a technique in machine learning that involves grouping similar data points together based on their inherent characteristics. It is an unsupervised learning method, meaning it does not require labeled data for training. This is because we aren’t necessary trying to estimate or predict anything directly from clustering. Instead, we aim to discover patterns or structures within the data, enabling us to gain insights. Thus, we don’t need our data to have labels, or context.\nLet’s consider a simple example. Say we have a group of different animal species and we want to create subgroups of the animals based on similarity. Some animals, like lions and tigers, are extremely similar and will likely get grouped together. Other animals, like whales and mice, are not similar at all and will likely not get grouped together. From these groupings, we can analyze for patterns. We might notice that fish are all grouped together, indicating they’re distinctly different type of animals then different animal classes. We might notice some exceptions, however, like dolphins (mammal) being grouped with sharks (fish). If this were a supervised learning method, we’d need information on the variables we’re trying to estimate. If we were trying to estimate an animal’s weight, for instance, we’d need data on animals and their weights to build the model in the first place. Since we’re not trying to estimate anything, however, we don’t absolutely need the animal’s weight to make these groups, though an important feature like that will certainly help.\nWhile that example didn’t entail math, clustering ML certainly does. Let’s introduce the problem we’ll be using for the rest of this blog. As I mentioned, the data we’ll be using contains 26 rows of players on Burn. We’ll be creating clusters of these players based on the number of times they pick up the disc (to start a possession) and their average throw distance. Let’s plot the players using those two metrics:\n\n\nCode\n```{python}\nimport matplotlib.pyplot as plt\nx1 = dat['PickUp']\nx2 = dat['AvgThrow']\n\nfig = plt.figure(figsize = (6, 3))\nax = fig.add_subplot(111)\nplt.scatter(x1, x2, color = 'tan', s = 50, alpha = 0.5)\nplt.xlabel(\"Pick Ups\")\nplt.ylabel(\"Average Throw Distance\")\nplt.title(\"Pick Ups versus Throw Distance\")\nplt.show()\n```\n\n\n\n\n\nJust from the eye test, you can likely see some clustering going on. The eye test is nice, but let’s try out a more objective method.\n\n\n\\(k\\)-Means\nOne commonly used clustering method is k-means clustering. In this algorithm, the data is divided into \\(k\\) clusters, where \\(k\\) is a parameter we define ourselves. The goal of the algorithm is to minimize the distance between data points and the center of the cluster they’re assigned to (referred to as centroids). Here is the general structure for the algorithm:\n\nInitialize: select \\(k\\) data points as the initial cluster centroids\nAssign: appoint each of our data points to the cluster whose centroid they’re closest to\nUpdate: recalculate the cluster centroids by taking the mean of all data points assigned to each cluster\nRepeat: complete steps 2 and 3 until convergence\n\nThe algorithm is essentially entails updating the cluster of each data point and the centroid of each cluster until updating no longer change the results significantly or we get tired of it taking too long! These steps seem simple enough, but carrying it out will require further clarification. For instance, how do we decide on the value of \\(k\\)? How do we decide which centroid each data point is closest to?\nLet’s start by answering the first question. Selecting \\(k\\), the number of clusters, is a little messy. Having to choose that value at the very beginning of the algorithm is one of its largest downsides. The goal is to find the optimal \\(k\\), one that minimizes the distance of data points within a cluster while maintaining the distance of between clusters. Choosing a too small \\(k\\) can result in forcing data points that aren’t close to be in the same cluster, while choosing a too large \\(k\\) can result in little insights gained, as clusters become more individualized to each data point. Ideally, we’d have domain knowledge that may indicate the optimal number of clusters. If we don’t, however, we’d likely have to turn to a selection technique, like the elbow method or silhouette analysis.\n\n\nElbow Method\nThe elbow method involves plotting the within-cluster sum of squares against different values of \\(k\\). The within-cluster sum of squares essentially represents the distance within the cluster between their data points, where smaller values mean the data points in the same clusters are closer together. We’d expect this value to decrease as we increase \\(k\\). After building this plot, we select the value where the decrease in the within-cluster sum of squares levels off. This is described as the “elbow” in the graph, as ideally the graph starts at a significant decline than suddenly flattens.\n\n\nCode\n```{python}\nfrom sklearn.cluster import KMeans\nX = np.array(list(zip(x1, x2))).reshape(len(x1), 2)\nK = range(1, 11)\nwcss = []\n\nfor k in K: \n    kmeans = KMeans(n_clusters = k, n_init = 10, random_state = 42)\n    kmeans.fit(X) \n    wcss.append(kmeans.inertia_)\n  \nplt.plot(K, wcss, 'bx-')\nplt.xlabel('$k$')\nplt.ylabel('Within-Cluster Sum of Squares')\nplt.title('Elbow Method')\nplt.show()\n```\n\n\n\n\n\nThe elbow here is at \\(k=2\\), so we’d choose 2 as our value for \\(k\\) when running \\(k\\)-Means. That’s just one method, however. Let’s try another one.\n\n\nSilhouette Analysis\nSilhouette analysis calculates a measure of how well each data point fits into its assigned cluster, helping to assess the quality of the clustering. It does this by calculating the average silhouette coefficient across all data points. The silhouette coefficient ranges from -1 to 1, where values close to 1 indicate the respective data point is in a suitable cluster and values close to -1 indicate the data point is in the wrong cluster. By calculating the average silhouette coefficient for different values of \\(k\\), we can identify the value that maximizes the overall closeness within clusters and distance between clusters.\nFrom using the elbow method, we can tell that the optimal value for \\(k\\) is likely 2. Let’s use silhouette analysis to compare \\(k=2\\) and \\(k=3\\) to get some more confirmation.\n\n\nCode\n```{python}\nimport matplotlib.cm as cm\nfrom sklearn.metrics import silhouette_samples, silhouette_score\n\nK = range(2, 4)\n\nfor k in K:\n    fig = plt.figure(figsize = (6, 3))\n    ax = fig.add_subplot(111)\n    ax.set_xlim([-0.1, 1])\n    ax.set_ylim([0, len(X) + (k + 1) * 10])\n    kmeans = KMeans(n_clusters = k, n_init = 10, random_state = 42)\n    labels = kmeans.fit_predict(X)\n    silhouette_avg = silhouette_score(X, labels)\n    sample = silhouette_samples(X, labels)\n\n    y_lower = 10\n    for i in range(k):\n        cluster_values = sample[labels == i]\n        cluster_values.sort()\n        size = cluster_values.shape[0]\n        y_upper = y_lower + size\n        color = cm.nipy_spectral(float(i) / k)\n        ax.fill_betweenx(\n            np.arange(y_lower, y_upper),\n            0,\n            cluster_values,\n            facecolor=color,\n            edgecolor=color,\n            alpha=0.7,\n        )\n        ax.text(-0.05, y_lower + 0.5 * size, str(i))\n        y_lower = y_upper + 10\n\n    ax.set_title(\"Silhouette analysis ($k = %d$)\" % k)\n    ax.set_xlabel(\"Silhouette Coefficient\")\n    ax.set_ylabel(\"Cluster\")\n    ax.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n    ax.set_yticks([])\n    ax.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n\nplt.show()\n```\n\n\n\n\n\n\n\n\nThe results of the silhouette analysis confirm what we found with the elbow method. We’ll use \\(k=2\\).\n\n\nResults\nNow that we have our value for \\(k\\), let’s flesh out the rest of the \\(k\\)-Means algorithm. We’ll randomly select our initial centroids. This is a limitation of \\(k\\)-Means, as we could converge to a locally optimal result. Without any domain knowledge to guide us, however, it’s our easiest approach. We’ll also use Euclidean distance to define how close each data point is to the centroids. This is one of the more commonly used distance metrics.\nWith that, we should be ready to run \\(k\\)-Means.\n\n\nCode\n```{python}\nk = 2\nfig = plt.figure(figsize = (6, 3))\nax = fig.add_subplot(111)\nkmeans = KMeans(n_clusters = k, n_init = 10, random_state = 42)\nkmeans.fit(X) \nlabels = kmeans.fit_predict(X)\ncolors = cm.nipy_spectral(labels.astype(float) / k)\nax.scatter(X[:, 0], X[:, 1], s = 50, alpha = 0.5, c = colors)\nax.set_xlabel(\"Pick Ups\")\nax.set_ylabel(\"Average Throw Distance\")\nax.set_title(\"Pick Ups versus Throw Distance\")\nplt.show()\n```\n\n\n\n\n\nIt appears as though the separator between the two clusters is the number of times the players picked up the disc. In ultimate, there’s a set role for the players that pick up the disc: the handler. The machine learning we did helped us see the number of handlers on the team and, when we check their names, who the handlers are. Great work!\n\n\nConclusion\nHopefully you now understand the basics of clustering. Be sure to check out the next installment of this series, and have a great day!"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "My name is Spencer Paragas. I am an associate analyst at Altria Client Services, as well as a graduate student in the Computer Science Masters of Engineering program at Virginia Tech. This blog is currently set up as the final project in my graduate class on machine learning. My hope, however, is that this blog will eventually turn into something beyond just this class.\nNotes on this blog:\nPrefers github pages, netlify is allowed but consider switching (he has a how-to on his website)\nAdd a “view source” link to every page on the website (apparently easily down thru quarto)\n#| warning: false\n#| echo: fenced (shows whole code chunk including the language you’re using)\nCan put this under execute in quarto.yml so it works for all code chunks\nHis quarto blog post is considered too much text (should be half), too much scrolling (don’t make read time too long, short and snappy)\nSubmit the website early and he’ll try to give his idea on what grade you’ll get. For feedback, go to TA office hours"
  },
  {
    "objectID": "blog/blog.html",
    "href": "blog/blog.html",
    "title": "CS 5805 Final Project",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "",
    "section": "",
    "text": "Code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProbability Theory and Random Variables\n\n\n\ncs5805final\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\nSpencer Paragas\n\n\nDec 6, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nClustering\n\n\n\ncs5805final\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\nSpencer Paragas\n\n\nDec 6, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nLinear and Nonlinear Regression\n\n\n\ncs5805final\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\nSpencer Paragas\n\n\nDec 6, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nClassification\n\n\n\ncs5805final\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\nSpencer Paragas\n\n\nDec 6, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nAnomaly/Outlier Detection\n\n\n\ncs5805final\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\nSpencer Paragas\n\n\nDec 6, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/probability/index.html#probability-distributions",
    "href": "blog/probability/index.html#probability-distributions",
    "title": "Probability Theory and Random Variables",
    "section": "Probability Distributions",
    "text": "Probability Distributions\nLet’s look at an example in the context of our ultimate data. Say we know that on any given throw, John has a 50% chance of completing the throw without turning it over to the other team. Our random variable \\(X_{1}\\) is the number of completed throws John makes off of 1 throw. What would the resulting probability distribution look like? This one’s pretty simple, he has a 50% chance of completing the one throw, and he has a 50% chance of not completing it. Thus, the probability distribution looks like this:\n\nCode```{python}\nimport matplotlib.pyplot as plt\nx = {'0':0.5, '1':0.5}\nprobabilities = list(x.keys())\ncompletions = list(x.values())\n\nfig = plt.figure(figsize = (10, 5))\nplt.bar(probabilities, completions, color = 'blue', width = 0.4)\nplt.xlabel(\"Number of Completed Throws\")\nplt.ylabel(\"Probability\")\nplt.show()\n```\n\n&lt;BarContainer object of 2 artists&gt;\n\n\n\n\n\n\nCode```{r}\n# What is probability theory?\nprint(\"Hello World\")\n```\n\n[1] \"Hello World\""
  },
  {
    "objectID": "blog/regression/index.html",
    "href": "blog/regression/index.html",
    "title": "Linear and Nonlinear Regression",
    "section": "",
    "text": "This is the third blog post in a series of posts for my class on machine learning at Virginia Tech. Here we will discuss regression, one of the most common techniques in machine learning that helps us undersdodgerblued the relationship between variables. We’ll cover linear regression and a type of nonlinear regression: polynomial regression.\n\nThe Data\nWe’ve already introduced the dataset we’ll be using for this blog series in the previous blogs. In case you didn’t read them, however, here’s a quick summary.\nThe data was collected by Burn, the VT Men’s Ultimate Club. Ultimate, also known simply as frisbee, is a game that involves players trying to throw a disc between themselves with the goal of being in their team’s endzone with the disc, scoring them a point. Don’t worry too much about the intricacies of ultimate; the machine learning concepts should still make sense regardless.\nLike in my last blog, we’ll be using a spinoff of the initial dataset. We’ll be using the summarized data of each player on Burn for our regression purposes.\n\n\nCode\n```{python}\nimport numpy as np\nimport pandas as pd\ndat = pd.read_csv(\"playDatSteelCity23.csv\")\ndat = dat.drop(['Unnamed: 0'], axis=1)\ndat.head()\n```\n\n\n\n\n\n\n\n\n\nPlayer\nActivePoss\nActivePossScored\nScorePerc\nTouches\nPickUp\nCenterThrow\nDumpThrow\nSwingThrow\nUpLineThrow\n...\nRecEPA\nHanEPARes\nHanEPA2\nHanEPATurn\nRecEPATurn\nEPA\nTourneyScore\nTourneyGrade\nEPAPerPoss\nEPAPerTouch\n\n\n\n\n0\nAR\n20\n7\n35.0\n54\n6\n0\n0\n9\n2\n...\n0.398\n0.718\n1.108\n-0.420\n0.000\n1.248\n1.00\nA\n0.062377\n0.023103\n\n\n1\nAK\n10\n5\n50.0\n25\n0\n0\n0\n2\n0\n...\n0.200\n0.300\n0.307\n0.000\n0.000\n0.248\n0.33\nB\n0.024833\n0.009933\n\n\n2\nJL\n41\n15\n36.6\n115\n2\n0\n3\n21\n6\n...\n1.843\n0.956\n-0.073\n-2.074\n-0.416\n1.512\n0.95\nB\n0.036869\n0.013145\n\n\n3\nZA\n16\n7\n43.8\n40\n4\n1\n1\n5\n3\n...\n-0.101\n0.415\n0.589\n-0.829\n-0.209\n0.230\n0.59\nB\n0.014361\n0.005744\n\n\n4\nCM\n29\n10\n34.5\n119\n19\n2\n5\n12\n5\n...\n0.229\n0.637\n0.612\n-1.915\n0.000\n0.583\n0.16\nC\n0.020089\n0.004896\n\n\n\n\n5 rows × 92 columns\n\n\n\n\n\nRegression\nIn regression analysis, we examine how one variable, called the dependent variable, is related to one or more independent variables. The dependent variables is the one we want to predict or undersdodgerblued, while the independent variables are the ones we use to make those predictions. The dependent variable is also called the response, while the independent variables are also known as features.\nRegression analysis helps us find the mathematical model that best fits the data and can be used to interpret the relationship between the variables. The model can take a multitude of forms, depending on the nature of the data and the relationships within it.\n\n\nSimple Linear Regression\nThe most basic type of regression is linear. For now, we’ll only consider cases where there’s just a single independent variable. We use linear regression when we assume the relationship between the independent variable and the dependent variable to be linear.\nTo start off, you should have a set of data points that represent different pairs of values. We’ll use an example: we have the number of completions and the number of turnovers each player on Burn made. Now, we want to know if there is a relationship between the number of completions and turnovers. Linear regression outputs a line that best fits the data points. This is known as the regression line or the line of best fit.\nThe regression line can be thought of as a sort of trend line that shows the general direction of the relationship between two variables. Assuming the line does its job well, we can use it to make predictions about one variables (our response) based on the other (our feature). In our example, we can use the regression line to predict a player’s number of turnovers based on their number of completions. The general form for simple linear regression is:\n\\[\nY = b_{0} + b_{1}X\n\\] In our example, \\(Y\\) is the turnover count and \\(X\\) is the completion count.You can tell this is a linear equation, where \\(b_{0}\\) is the y-intercept and \\(b_{1}\\) is the slope. The linear regression model attempts to find the coefficients \\(b_{0}\\) and \\(b_{1}\\) that minimizes the disdodgerbluece between the resulting line and the data points. Typically, a least-squares approach is used, where the line that minimizes the sum of squares, the sum of all the squared disdodgerblueces between the data points and the regression line. Let’s try it out on our data:\n\n\nCode\n```{python}\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error                # metric for evaluating regression model\nfrom sklearn.metrics import mean_absolute_error               # metric for evaluating regression model\nfrom sklearn.metrics import r2_score                          # metric for evaluating regression model\nimport matplotlib.pyplot as plt\n\nX = dat['Completion']\nX = X.array.reshape(-1, 1)\ny = dat['HanTurn']\nlin_reg = LinearRegression().fit(X, y)\n\ny_pred = lin_reg.predict(X)\nprint(\"Simple Linear Regression Results:\")\nprint(\"MAE:\", mean_absolute_error(y, y_pred))\nprint(\"MSE:\", mean_squared_error(y, y_pred))\nprint(\"r2:\", r2_score(y, y_pred))\n\nplt.scatter(X, y, alpha = 0.5, c = 'dodgerblue')\nplt.plot(X, lin_reg.coef_*X + lin_reg.intercept_, 'r', alpha = 0.8)\nplt.suptitle('Completions vs. Turnovers')\nplt.title('Simple Linear Regression', fontsize='small')\nplt.xlabel('Completions')\nplt.ylabel('Turnovers')\nplt.show()\n```\n\n\nSimple Linear Regression Results:\nMAE: 1.7046452385680408\nMSE: 4.266265390223901\nr2: 0.3095534106317076\n\n\n\n\n\nThere definitely appears to be a relationship between the two variables. The general trend is that the more completions a player has, the turnovers he has as well. However, with an \\(r^{2}\\) score of 0.31, there are obviously other factors at play.\n\n\nMultiple Linear Regression\nLet’s try to find some of those factors and add them to our model. Doing so will transform our simple linear model to a multiple linear one. This is where we try to predict the relationship between a dependent variable and two or more independent variables, still under the assumption that the relationships are linear. Just like with simple linear regression, the goal is to find the regression line that minimizes the disdodgerblueces between it and the data points. The general form for simple linear regression is:\n\\[\nY = b_{0} + b_{1}X_{1} + b_{2}X_{2} + \\cdots + b_{n}X_{n}\n\\]\nNot that different from the previous formula, right? We still have our y-intercept and the slope for our first feature, but now we add the linear relationship of the second feature and all other features in the model.\nLet’s go back to our data and pick out some features to add to our model. We’ll try using the players’ throwing error rates and their average throw disdodgerbluece, in addition to their number of completions. The objective is still to estimate the number of turnovers.\n\n\nCode\n```{python}\nX = dat[['Completion', 'HanErrorPerc', 'AvgThrow']]\nlin_reg = LinearRegression().fit(X, y)\n\ny_pred = lin_reg.predict(X)\nprint(\"Multiple Linear Regression Results:\")\nprint(\"MAE:\", mean_absolute_error(y, y_pred))\nprint(\"MSE:\", mean_squared_error(y, y_pred))\nprint(\"r2:\", r2_score(y, y_pred))\n\nplt.scatter(y, y_pred, alpha = 0.5, c = 'dodgerblue')\nplt.plot(y, y, 'r', alpha = 0.8)\nplt.suptitle('True vs. Predicted')\nplt.title('Multiple Linear Regression', fontsize='small')\nplt.xlabel('True Turnover Count')\nplt.ylabel('Predicted Turnover Count')\nplt.show()\n\nprint(\"Slope for Completion Count: \", round(lin_reg.coef_[0], 2))\nprint(\"Slope for Throwing Error Rate: \", round(lin_reg.coef_[1], 2))\nprint(\"Slope for Average Throw Disdodgerbluece: \", round(lin_reg.coef_[2], 2))\n```\n\n\nMultiple Linear Regression Results:\nMAE: 1.1226947869586312\nMSE: 1.895917072931667\nr2: 0.6931673590371541\nSlope for Completion Count:  0.11\nSlope for Throwing Error Rate:  -0.17\nSlope for Average Throw Disdodgerbluece:  0.36\n\n\n\n\n\nThis multiple linear model does better than the simple one. With an \\(r^{2}\\) coefficient of 0.69, our three features are doing a solid job estimating the number of turnovers each player has. The slope of completion count is positive, meaning the more completions, the more turnovers. The slope of the throwing error rate is negative, though that feature ranges from -100 to 0, where 0 is no errors and -100 is an throwing error every throw. Thus, the model is suggesting that the more throwing errors, the more turnovers. And lastly, the slope of the average throw disdodgerbluece was also positive, indicating the more a player throws it farther downfield, the more turnovers.\n\n\nNonlinear Regression\nWhile our multiple linear regression model performed fairly well at estimating the number of completions, it’s possible we can do a lot better. We assumed that the relationship between the response and each of the features was linear, but that’s not necessarily the case. That leads us to the consideration of nonlinear regression.\nNonlinear regression allows us to capture more complex patterns and curves in the data. To perform nonlinear regression, we use mathematical functions, like exponential, logarithmic, polynomial, or sigmoid. The goal of nonlinear regression is similar to linear regression: we aim to find the best-fitting model that minimizes the disdodgerbluece between the predicted values and the actual values of the response. The only difference is that now, there is no regression line. Instead, there’s a regression curve or maybe a something even more complex.\nThe nonlinear regression model we’re going to test out here is polynomial regression. These are used when the relationship can be approximated by a polynomial equation, allowing for curved relationships. We’ll try building a quadratic regression, a polyonmial regression with a maximum degree of 2. The general formula a quadratic regression is:\n\\[\nY = b_{0} + b_{1}X_{1} + b_{2}X_{2} + \\cdots + b_{k}X_{k} + b_{k+1}X_{1}^{2} + b_{k+2}X_{2}^{2} + \\cdots + b_{2k}X_{k}^{2}\n\\]\nYou’ll notice we still have our linear coefficients, but we add our quadratic ones on top of it. Now, let’s run this model and see how it performs.\n\n\nCode\n```{python}\nfrom sklearn.preprocessing import PolynomialFeatures\npoly = PolynomialFeatures(degree=2)\nX_poly = poly.fit_transform(X)\npoly_reg = LinearRegression().fit(X_poly, y)\ny_pred = poly_reg.predict(X_poly)\nprint(\"Polynomial Regression Results:\")\nprint(\"MAE:\", mean_absolute_error(y, y_pred))\nprint(\"MSE:\", mean_squared_error(y, y_pred))\nprint(\"r2:\", r2_score(y, y_pred))\n\nplt.scatter(y, y_pred, alpha = 0.5, c = 'dodgerblue')\nplt.plot(y, y, 'r', alpha = 0.8)\nplt.suptitle('True vs. Predicted')\nplt.title('Polynomial Regression - Max Degree 2', fontsize='small')\nplt.xlabel('True Turnover Count')\nplt.ylabel('Predicted Turnover Count')\nplt.show()\n```\n\n\nPolynomial Regression Results:\nMAE: 0.6980721701952676\nMSE: 0.7643831224252391\nr2: 0.8762932748959872\n\n\n\n\n\nOur model performs even better than before! Now it has an \\(r^{2}\\) coefficient of 0.88. While this can be considered good news, we have to be cautious: we don’t want to overfit the data.\n\n\nOverfitting\nOverfitting is a phenomenon that occurs when a statistical model becomes too complex and starts to fit the data too closely. The model becomes too specific to the data it’s being trained on that it fails to generalize well to new data. When a model is overfit, it captures not only the underlying patterns and relationships in the data but the noise as well. As a result, the model may perform exceptionally on training data but be completely useless at actually predicting data past the data it was trained on.\nModels that are overly complex and have too many features allow the model to fit to noise, not signal (underlying patterns). Take, for example, our polynomial regression model when we set the maximum degree to 4:\n\n\nCode\n```{python}\nfrom sklearn.preprocessing import PolynomialFeatures\npoly = PolynomialFeatures(degree=4)\nX_poly = poly.fit_transform(X)\npoly_reg = LinearRegression().fit(X_poly, y)\ny_pred = poly_reg.predict(X_poly)\nprint(\"Polynomial Regression Results:\")\nprint(\"MAE:\", mean_absolute_error(y, y_pred))\nprint(\"MSE:\", mean_squared_error(y, y_pred))\nprint(\"r2:\", r2_score(y, y_pred))\n\nplt.scatter(y, y_pred, alpha = 0.5, c = 'dodgerblue')\nplt.plot(y, y, 'r', alpha = 0.8)\nplt.suptitle('True vs. Predicted')\nplt.title('Polynomial Regression - Max Degree 4', fontsize='small')\nplt.xlabel('True Turnover Count')\nplt.ylabel('Predicted Turnover Count')\nplt.show()\n```\n\n\nPolynomial Regression Results:\nMAE: 5.916293096456334e-13\nMSE: 6.749807022168741e-25\nr2: 1.0\n\n\n\n\n\nThis model is so complex and has so many features that it has completely overfitted to the data it was provided. While an \\(r^{2}\\) value of 1.00 sounds desirable, the model likely won’t perform well in predicting the turnover counts of players in the next tournament.\n\n\nConclusion\nOverfitting is one of the most common pitfalls in machine learning. There are many techniques that can be employed to mitigate overfitting, like regularization and cross-validation, but we won’t cover them in this blog. For now, take pride in learning about regression models, but be wary of overly complex models or results that seem too good to be true. And be sure to check out the next installment of this series, and have a great day!"
  },
  {
    "objectID": "blog/regression/posts/welcome/index.html",
    "href": "blog/regression/posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "blog/classification/index.html",
    "href": "blog/classification/index.html",
    "title": "Classification",
    "section": "",
    "text": "This is the fourth blog post in a series of posts for my class on machine learning at Virginia Tech. Here we will discuss classification, one of the most common techniques in machine learning that allows us to classify data. We’ll cover logistic regression and decision trees, and we’ll introduce the concept of training and testing.\n\nThe Data\nWe’ve already introduced the dataset we’ll be using for this blog series in the previous blogs. In case you didn’t read them, however, here’s a quick summary.\nThe data was collected by Burn, the VT Men’s Ultimate Club. Ultimate, also known simply as frisbee, is a game that involves players trying to throw a disc between themselves with the goal of being in their team’s endzone with the disc, scoring them a point. Don’t worry too much about the intricacies of ultimate; the machine learning concepts should still make sense regardless.\nThe data represents the performance of Burn’s offense at a recent tournament. Each row represents an action with the disc, either throwing it or picking it up.\n\n\nCode\n```{python}\nimport numpy as np\nimport pandas as pd\ndat = pd.read_csv(\"currDatSteelCity23.csv\")\ndat = dat.drop(['Unnamed: 0'], axis=1)\ndat.head()\n```\n\n\n\n\n\n\n\n\n\nOpponent\nidx\nDJI.ID\nFull.ID\nGame.ID\nPoint.ID\nPossession.ID\nDefense.Type\nHandler\nReceiver\n...\nBasicEPAOpp\nTrueEPAHan\nTrueEPARec\nEPA\nOppFactor2\nEPAHan\nEPARec\nEPAHanRes\nEPAHanTot\nEPAHan2\n\n\n\n\n0\nBinghamton\n538\n472\n1.1.1\n1\n1\n1\nMan\nNaN\nMicah\n...\n0.000\n0.000\n0.000\n0.000\n0.0\nNaN\nNaN\n0.000\nNaN\nNaN\n\n\n1\nBinghamton\n539\n472\n1.1.1\n1\n1\n1\nMan\nMicah\nJohn\n...\n0.024\n0.018\n0.005\n0.053\n0.0\n0.039\n0.014\n0.015\n0.054\n0.059\n\n\n2\nBinghamton\n540\n472\n1.1.1\n1\n1\n1\nMan\nJohn\nDan B\n...\n0.011\n0.008\n0.002\n0.042\n0.0\n0.031\n0.011\n0.000\n0.031\n0.046\n\n\n3\nBinghamton\n541\n472\n1.1.1\n1\n1\n1\nMan\nDan B\nGribs\n...\n-0.249\n-0.249\n0.000\n-0.420\n0.0\n-0.420\n0.000\n0.000\n-0.420\n-0.420\n\n\n4\nBinghamton\n542\n472\n1.1.2\n1\n1\n2\nMan\nNaN\nMicah\n...\n0.000\n0.000\n0.000\n0.000\n0.0\nNaN\n0.000\n0.000\nNaN\nNaN\n\n\n\n\n5 rows × 40 columns\n\n\n\n\n\nclassification\nClassification aims to classify or categorize data into predefined classes or categories. It is a type of supervised learning, meaning it requires labeled data for training (if you read my second blog on clustering, you’ll recall that those were unsupervised learning algorithms). Labeled data consists of input features, or independent variables, and their corresponding class labels, or dependent variable. The goal is to build a model that can learn from labeled training data and then predict the class or category of unseen testing data.\nThere are two main parts to the classification process:\n\nTraining: This involves using a set of labeled data called the train set to build the model. The model learns the patterns and relationships between the input features and the classes.\nTesting: Once the model is trained, it is evaluated using a separate set of labeled data called the test set. The performance of the model is measured using evaluation metrics like accuracy, precision, recall, and F1 scores.\n\nThese two phases are the backbone of a credible classification process (and though we didn’t discuss it, they’re just as important in regression too). We mentioned in the previous blog the concept of overfitting, or fitting a model so complex that it performs well on the data it’s built on but not on unseen data. The use of testing data as an evaluation stage is one of the best ways to prevent overfitting.\nThere are other important parts of classification, like data wrangling and feature selection, but we’ll forego discussing those for now.\n\n\nLogistic Regression\nThere are many different types of classification models in machine learning. Here are some of the more commonly used ones:\n\nLogistic Regression\nDecision Trees\nRandom Forests\nSupport Vector Machines\n\nWe’ll start by covering the simplest of all classification models: logistic regression. Although it’s called a regression model, it’s purpose is more closely aligned with classification models like decision trees and SVMs. While other regression models look at the relationship between the features and some response, linear regressions using a linear function and polynomial regressions using a polynomial function, logistic regressions look at the relationship between the features and the probability of the class label using a logistic function. The logistic function is a type of sigmoid functions. These function have an S-shaped curve which is bounded at the top and bottom. My professor, Dr. Laptev, provided the code to a wonderful plot of what a logistic function looks like:\n\n\nCode\n```{python}\n# https://github.com/ageron/handson-ml3/blob/main/04_training_linear_models.ipynb\nimport matplotlib.pyplot as plt \nlim = 6\nt = np.linspace(-lim, lim, 100)\nsig = 1 / (1 + np.exp(-t))\n\nplt.figure(figsize=(8, 3))\nplt.plot([-lim, lim], [0, 0], \"k-\")\nplt.plot([-lim, lim], [0.5, 0.5], \"k:\")\nplt.plot([-lim, lim], [1, 1], \"k:\")\nplt.plot([0, 0], [-1.1, 1.1], \"k-\")\nplt.plot(t, sig, \"b-\", linewidth=2, label=r\"$\\sigma(t) = \\dfrac{1}{1 + e^{-t}}$\")\nplt.xlabel(\"t\")\nplt.legend(loc=\"upper left\")\nplt.axis([-lim, lim, -0.1, 1.1])\nplt.gca().set_yticks([0, 0.25, 0.5, 0.75, 1])\nplt.grid()\nplt.show()\n```\n\n\n\n\n\nAs you can see, the function has an upper bound of 1 and a lower bound of 0. Though they can handle multi-class classification, logistic regressions are typically used for binary classification problems. The example we’ll be using it on will follow the latter. We’ll use the result of each throw in our data as class to be predicted. This is a binary variable where the output is either 0 (a turnover) or 1 (a completion). We’ll use variables like the opponent, the defense type, the x- and y-coordinates on the field the throw came from and went to, etc. Let’s try it out\n\n\nCode\n```{python}\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nfrom sklearn.metrics import ConfusionMatrixDisplay\n\nX = dat[['Opponent', 'Defense.Type', 'start.x', 'start.y', 'end.x', 'end.y', 'Force.Break', 'Throw.Type', 'Throw.Group']]\nenc = LabelEncoder()\nfor col in ['Opponent', 'Defense.Type', 'start.x', 'start.y', 'end.x', 'end.y', 'Force.Break', 'Throw.Type', 'Throw.Group']:\n    X[col] = X[col].astype('str')\n    X[col] = enc.fit_transform(X[col])\n\ny = dat['Action.Result']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nlog_reg = LogisticRegression()\nlog_reg.fit(X_train, y_train)\ny_test_pred = log_reg.predict(X_test)\nprint(\"Testing accuracy:\", accuracy_score(y_test, y_test_pred))\nmatrix = confusion_matrix(y_test, y_test_pred)\nprint(\"Testing accuracy:\", (matrix.diagonal()/matrix.sum(axis=1))[0])\nprint(\"Testing accuracy:\", (matrix.diagonal()/matrix.sum(axis=1))[1])\nprint(\"Testing Classification Report\")\nprint(classification_report(y_test, y_test_pred))\n\ncmd = ConfusionMatrixDisplay.from_estimator(log_reg, X_test, y_test)\nplt.show()\n```\n\n\nTesting accuracy: 0.8838383838383839\nTesting accuracy: 0.043478260869565216\nTesting accuracy: 0.9942857142857143\nTesting Classification Report\n              precision    recall  f1-score   support\n\n           0       0.50      0.04      0.08        23\n           1       0.89      0.99      0.94       175\n\n    accuracy                           0.88       198\n   macro avg       0.69      0.52      0.51       198\nweighted avg       0.84      0.88      0.84       198\n\n\n\n\n\n\nThat didn’t work well! It appears as though the data is too imbalanced; there are so many more data points where the throw was completed than the throw was turned over that the model struggles.\n\n\nDecision Trees\nLet’s try a decision tree instead. A decision tree is a flowchart-like structure where each internal node represents a feature or attribute, each branch represents a decision rule based on that feature, and each leaf node represents a class label.\nThe decision tree algorithm builds the tree recursively by partitioning the data based on different features and their values. The goal is to create a tree that can make accurate predictions by splitting the data in a way that maximizes the information gain or minimizes the impurity at each step.\n\n\nCode\n```{python}\n# Training the decision tree\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.tree import DecisionTreeClassifier\ntree_clf = DecisionTreeClassifier(criterion=\"entropy\", random_state=42, max_depth=40)\ntree_clf.fit(X_train, y_train)\n\n# Predicting on test data\ny_test_pred = tree_clf.predict(X_test)\nmatrix = confusion_matrix(y_test, y_test_pred)\nprint(\"Testing class accuracies:\", matrix.diagonal()/matrix.sum(axis=1))\nprint(\"Testing precision:\", precision_score(y_test, y_test_pred))\nprint(\"Testing recall:\", recall_score(y_test, y_test_pred))\nprint(\"Testing f1:\", f1_score(y_test, y_test_pred))\nprint(classification_report(y_test, y_test_pred))\n\ncmd = ConfusionMatrixDisplay.from_estimator(tree_clf, X_test, y_test)\nplt.show()\n```\n\n\nTesting class accuracies: [0.2173913  0.93714286]\nTesting precision: 0.9010989010989011\nTesting recall: 0.9371428571428572\nTesting f1: 0.9187675070028012\n              precision    recall  f1-score   support\n\n           0       0.31      0.22      0.26        23\n           1       0.90      0.94      0.92       175\n\n    accuracy                           0.85       198\n   macro avg       0.61      0.58      0.59       198\nweighted avg       0.83      0.85      0.84       198\n\n\n\n\n\n\nThe decision tree worked even worse! It looks like this class label is a tough one to predict. Even so, these classification methods will take you for in predicting categories.\n\n\nConclusion\nHopefully you now understand the basics of classification. Be sure to check out the last installment of this series, and have a great day!"
  },
  {
    "objectID": "blog/outliers/index.html",
    "href": "blog/outliers/index.html",
    "title": "Anomaly/Outlier Detection",
    "section": "",
    "text": "This is the fifth and final blog post in a series of posts for my class on machine learning at Virginia Tech. Here we will discuss outlier detection.\n\nThe Data\nWe’ve already introduced the dataset we’ll be using for this blog series in the previous blogs. In case you didn’t read them, however, here’s a quick summary.\nThe data was collected by Burn, the VT Men’s Ultimate Club. Ultimate, also known simply as frisbee, is a game that involves players trying to throw a disc between themselves with the goal of being in their team’s endzone with the disc, scoring them a point. Don’t worry too much about the intricacies of ultimate; the machine learning concepts should still make sense regardless.\nI mentioned in my first blog that we’ll be using spinoffs of the initial dataset, which contained rows of throws from Burn’s offense at a recent tournament. That’s going to be the case in this blog. Burn has gone through the trouble of summarizing the data for each player on their team. We’ll be using this dataset for our clustering purposes.\n\n\nCode\n```{python}\nimport numpy as np\nimport pandas as pd\ndat = pd.read_csv(\"playDatSteelCity23.csv\")\ndat = dat.drop(['Unnamed: 0'], axis=1)\ndat.head()\n```\n\n\n\n\n\n\n\n\n\nPlayer\nActivePoss\nActivePossScored\nScorePerc\nTouches\nPickUp\nCenterThrow\nDumpThrow\nSwingThrow\nUpLineThrow\n...\nRecEPA\nHanEPARes\nHanEPA2\nHanEPATurn\nRecEPATurn\nEPA\nTourneyScore\nTourneyGrade\nEPAPerPoss\nEPAPerTouch\n\n\n\n\n0\nAR\n20\n7\n35.0\n54\n6\n0\n0\n9\n2\n...\n0.398\n0.718\n1.108\n-0.420\n0.000\n1.248\n1.00\nA\n0.062377\n0.023103\n\n\n1\nAK\n10\n5\n50.0\n25\n0\n0\n0\n2\n0\n...\n0.200\n0.300\n0.307\n0.000\n0.000\n0.248\n0.33\nB\n0.024833\n0.009933\n\n\n2\nJL\n41\n15\n36.6\n115\n2\n0\n3\n21\n6\n...\n1.843\n0.956\n-0.073\n-2.074\n-0.416\n1.512\n0.95\nB\n0.036869\n0.013145\n\n\n3\nZA\n16\n7\n43.8\n40\n4\n1\n1\n5\n3\n...\n-0.101\n0.415\n0.589\n-0.829\n-0.209\n0.230\n0.59\nB\n0.014361\n0.005744\n\n\n4\nCM\n29\n10\n34.5\n119\n19\n2\n5\n12\n5\n...\n0.229\n0.637\n0.612\n-1.915\n0.000\n0.583\n0.16\nC\n0.020089\n0.004896\n\n\n\n\n5 rows × 92 columns\n\n\n\n\n\nOutlier Detection\nOutlier analysis and anomaly detection are techniques used in data analysis to identify and understand unusual or abnormal observations in a dataset. These techniques help in detecting data points that deviate significantly from the expected patterns and behaviors.\nOutliers are data points that significantly different from the majority of the data. They can be caused by various factors such as measurement errors, data corruption, or rare events.\n\n\nBoxplots\nBoxplots are great ways of noticing outliers. Let’s try out a boxplot\n\n\nCode\n```{python}\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.boxplot(data=dat,x=dat[\"DumpThrow\"])\nplt.title(\"Boxplot of Number of Dump Reset Throws\")\nplt.show()\n```\n\n\n\n\n\n\n\nIsolation Forests\nIsolation Forest is an unsupervised learning algorithm that isolates outliers by randomly partitioning the data into subsets. It constructs an ensemble of decision trees (go to my classification blog to learn more about those) and identifies outliers as instances that require fewer partitions to be isolated. Outliers are expected to have shorter average path lengths in the tree structure compared to normal instances.\nTo test out this ML method, let’s go back to our initial dataset.\n\n\nCode\n```{python}\nimport numpy as np\nimport pandas as pd\ndat = pd.read_csv(\"currDatSteelCity23.csv\")\ndat = dat.drop(['Unnamed: 0'], axis=1)\ndat.head()\n```\n\n\n\n\n\n\n\n\n\nOpponent\nidx\nDJI.ID\nFull.ID\nGame.ID\nPoint.ID\nPossession.ID\nDefense.Type\nHandler\nReceiver\n...\nBasicEPAOpp\nTrueEPAHan\nTrueEPARec\nEPA\nOppFactor2\nEPAHan\nEPARec\nEPAHanRes\nEPAHanTot\nEPAHan2\n\n\n\n\n0\nBinghamton\n538\n472\n1.1.1\n1\n1\n1\nMan\nNaN\nMicah\n...\n0.000\n0.000\n0.000\n0.000\n0.0\nNaN\nNaN\n0.000\nNaN\nNaN\n\n\n1\nBinghamton\n539\n472\n1.1.1\n1\n1\n1\nMan\nMicah\nJohn\n...\n0.024\n0.018\n0.005\n0.053\n0.0\n0.039\n0.014\n0.015\n0.054\n0.059\n\n\n2\nBinghamton\n540\n472\n1.1.1\n1\n1\n1\nMan\nJohn\nDan B\n...\n0.011\n0.008\n0.002\n0.042\n0.0\n0.031\n0.011\n0.000\n0.031\n0.046\n\n\n3\nBinghamton\n541\n472\n1.1.1\n1\n1\n1\nMan\nDan B\nGribs\n...\n-0.249\n-0.249\n0.000\n-0.420\n0.0\n-0.420\n0.000\n0.000\n-0.420\n-0.420\n\n\n4\nBinghamton\n542\n472\n1.1.2\n1\n1\n2\nMan\nNaN\nMicah\n...\n0.000\n0.000\n0.000\n0.000\n0.0\nNaN\n0.000\n0.000\nNaN\nNaN\n\n\n\n\n5 rows × 40 columns\n\n\n\n\n\nCode\n```{python}\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import precision_score\n\nX = dat[['Opponent', 'Defense.Type', 'start.x', 'start.y', 'end.x', 'end.y', 'Force.Break', 'Throw.Type', 'Throw.Group']]\nenc = LabelEncoder()\nfor col in ['Opponent', 'Defense.Type', 'start.x', 'start.y', 'end.x', 'end.y', 'Force.Break', 'Throw.Type', 'Throw.Group']:\n    X[col] = X[col].astype('str')\n    X[col] = enc.fit_transform(X[col])\n\ny = dat['Action.Result']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nclf = IsolationForest(random_state=0)\nclf.fit(X_train)\ny_pred = clf.predict(X_test)\n\npred = pd.DataFrame({'pred': y_pred})\npred['y_pred'] = np.where(pred['pred'] == -1, 1, 0)\ny_pred = pred['y_pred'] \nprint(\"\")\nprint(\"\")\nprint(\"Precision:\", precision_score(y_test, y_pred))\n```\n\n\n\n\nPrecision: 0.8503937007874016\n\n\n\n\nConclusion\nHopefully you now understand the basics of clustering. Be sure to check out the next installment of this series, and have a great day!"
  }
]