[
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Clustering",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\n# What are some clustering methods?"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About",
    "section": "",
    "text": "My name is Spencer Paragas. I am an associate analyst at Altria Client Services, as well as a graduate student in the Computer Science Masters of Engineering program at Virginia Tech. This blog is currently set up as the final project in my graduate class on machine learning. My hope, however, is that this blog will eventually turn into something beyond just this class.\nNotes on this blog:\nPrefers github pages, netlify is allowed but consider switching (he has a how-to on his website)\nAdd a “view source” link to every page on the website (apparently easily down thru quarto)\n#| warning: false\n#| echo: fenced (shows whole code chunk including the language you’re using)\nCan put this under execute in quarto.yml so it works for all code chunks\nHis quarto blog post is considered too much text (should be half), too much scrolling (don’t make read time too long, short and snappy)\nSubmit the website early and he’ll try to give his idea on what grade you’ll get. For feedback, go to TA office hours"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "CS 5805 Final Project",
    "section": "",
    "text": "Probability Theory and Random Variables\n\n\n\n\n\n\n\ncs5805final\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 30, 2023\n\n\nSpencer Paragas\n\n\n\n\n\n\n  \n\n\n\n\nClustering\n\n\n\n\n\n\n\ncs5805final\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 30, 2023\n\n\nSpencer Paragas\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Probability Theory and Random Variables",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2\n\n\n\n# What is probability theory?\nprint(\"Hello World\")\n\n[1] \"Hello World\""
  },
  {
    "objectID": "blog/welcome/index.html",
    "href": "blog/welcome/index.html",
    "title": "Clustering",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\n\nCode\n```{python}\n# What are some clustering methods?\n2+2\n```\n\n\n4"
  },
  {
    "objectID": "blog/post-with-code/index.html",
    "href": "blog/post-with-code/index.html",
    "title": "Probability Theory and Random Variables",
    "section": "",
    "text": "This is a post with executable code.\n\nCode```{r}\n1 + 1\n```\n\n[1] 2\n\n\n\nCode```{r}\n# What is probability theory?\nprint(\"Hello World\")\n```\n\n[1] \"Hello World\""
  },
  {
    "objectID": "blog/probability theory/index.html",
    "href": "blog/probability theory/index.html",
    "title": "Probability Theory and Random Variables",
    "section": "",
    "text": "This is a post with executable code.\n\nCode```{r}\n1 + 1\n```\n\n[1] 2\n\n\n\nCode```{r}\n# What is probability theory?\nprint(\"Hello World\")\n```\n\n[1] \"Hello World\""
  },
  {
    "objectID": "blog/welcome/cluster.html",
    "href": "blog/welcome/cluster.html",
    "title": "Clustering",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\n\nCode\n```{python}\n# What are some clustering methods?\n2+2\n```\n\n\n4"
  },
  {
    "objectID": "blog/probability/index.html",
    "href": "blog/probability/index.html",
    "title": "Probability Theory and Random Variables",
    "section": "",
    "text": "This is the first blog post in a series of posts for my class on machine learning at Virginia Tech. Here we will discuss probability theory, which is the foundation to the later machine learning posts in this series. While probability theory is a massive branch of mathematics that can’t be fully covered in one blog, we’ll go over topics like random variables, conditional probability, and independence.\n\nThe Data\nBefore that, however, let’s introduce the dataset we’ll be using for the entirety of this blog series. The data was collected by Burn, the VT Men’s Ultimate Club. Ultimate, also known simply as frisbee, is a game that involves players trying to throw a disc between themselves with the goal of being in their team’s endzone with the disc, scoring them a point. For many of you, this game may be foreign, making some of the variables confusing. While you don’t need to know most of the context to understand the machine learning concepts shown, I’ll do my best to explain the important parts as succinctly as possible.\nThe data represents the performance of the ultimate club at Steel City, a tournament in Pittsburgh they went to this Fall. Only data on Burn’s offense (when they had the disc) was collected. In the initial dataset, each row represents an action with the disc, either throwing it or picking it up. The features include variables like the opponents they were facing, who threw it to who, the type of throw it was, and the outcome of the throw. In total the dataset is \\(986 \\times 40\\), though spinoffs of the dataset will be used later on in the series.\n\n\nCode\n```{python}\nimport numpy as np\nimport pandas as pd\ndat = pd.read_csv(\"currDatSteelCity23.csv\")\ndat = dat.drop(['Unnamed: 0'], axis=1)\ndat.head()\n```\n\n\n\n\n\n\n\n\n\nOpponent\nidx\nDJI.ID\nFull.ID\nGame.ID\nPoint.ID\nPossession.ID\nDefense.Type\nHandler\nReceiver\n...\nBasicEPAOpp\nTrueEPAHan\nTrueEPARec\nEPA\nOppFactor2\nEPAHan\nEPARec\nEPAHanRes\nEPAHanTot\nEPAHan2\n\n\n\n\n0\nBinghamton\n538\n472\n1.1.1\n1\n1\n1\nMan\nNaN\nMicah\n...\n0.000\n0.000\n0.000\n0.000\n0.0\nNaN\nNaN\n0.000\nNaN\nNaN\n\n\n1\nBinghamton\n539\n472\n1.1.1\n1\n1\n1\nMan\nMicah\nJohn\n...\n0.024\n0.018\n0.005\n0.053\n0.0\n0.039\n0.014\n0.015\n0.054\n0.059\n\n\n2\nBinghamton\n540\n472\n1.1.1\n1\n1\n1\nMan\nJohn\nDan B\n...\n0.011\n0.008\n0.002\n0.042\n0.0\n0.031\n0.011\n0.000\n0.031\n0.046\n\n\n3\nBinghamton\n541\n472\n1.1.1\n1\n1\n1\nMan\nDan B\nGribs\n...\n-0.249\n-0.249\n0.000\n-0.420\n0.0\n-0.420\n0.000\n0.000\n-0.420\n-0.420\n\n\n4\nBinghamton\n542\n472\n1.1.2\n1\n1\n2\nMan\nNaN\nMicah\n...\n0.000\n0.000\n0.000\n0.000\n0.0\nNaN\n0.000\n0.000\nNaN\nNaN\n\n\n\n\n5 rows × 40 columns\n\n\n\n\n\nRandom Variables\nA random variable (RV) is a variable of unknown value that represents the numerical outcome of some process. This is vague, but essentially an RV can’t be known (hence it’s a variable), has to be numerical, and has to have some kind of context to it. RVs are typically represented by capitalized letters, like \\(X\\) or \\(Y\\). Some examples include the number of coins that flip to head out of 2 or the height of a person. The former example is one where the RV is discrete: it can only be a distinct value like 0, 1, or 2. The latter example is one where the RV is continuous: it can be any of the infinite values within a range, like any number between 0 and 1000.\nThe significance of random variables in the world of probability theory and, more broadly, statistics in general is that RVs have probability distributions that allow us to calculate the probabilities of RVs. This process has many applications, where people try to understand and estimate real world RVs.\n\n\nProbability Distributions\nLet’s look at an example in the context of our ultimate data. Say we know that on any given throw, John has a 50% chance of completing the throw without turning it over to the other team. Our random variable \\(X_{1}\\) is the number of completed throws John makes off of 1 throw. This is RV is discrete. What would the resulting probability distribution look like? This one’s pretty simple, he has a 50% chance of completing the one throw, and he has a 50% chance of not completing it. Thus, the probability distribution looks like this:\n\n\nCode\n```{python}\nimport matplotlib.pyplot as plt\nx = {'0':0.5, '1':0.5}\ncompletions = list(x.keys())\nprobabilities = list(x.values())\n\nfig = plt.figure(figsize = (6, 3))\nplt.bar(completions, probabilities, color = 'cornflowerblue', width = 0.4)\nplt.xlabel(\"Number of Completed Throws\")\nplt.ylabel(\"Probability\")\nplt.title(\"Probability Distribution for $X_1$\")\nplt.show()\n```\n\n\n\n\n\nWhat about the probability distribution of the random variable \\(X_{2}\\), the number of completed throws John makes off of 2 throws? Each throw has an equal chance of being a completion or an incompletion (50%). Half of the time John will complete the first throw, and half of those times John will complete the second throw. That gives the probability of John completing 2 out of 2 throws 0.25. The same logic can give you the probability of 0 completions. Thus, the probability distribution for \\(X_{2}\\) looks like this:\n\n\nCode\n```{python}\nx = {'0':0.25, '1':0.5, '2':0.25}\ncompletions = list(x.keys())\nprobabilities = list(x.values())\n\nfig = plt.figure(figsize = (6, 3))\nplt.bar(completions, probabilities, color = 'cornflowerblue', width = 0.4)\nplt.xlabel(\"Number of Completed Throws\")\nplt.ylabel(\"Probability\")\nplt.title(\"Probability Distribution for $X_2$\")\nplt.show()\n```\n\n\n\n\n\n\\(X_{1}\\) and \\(X_{2}\\) are examples of random variables with a binomial distribution. This distribution represent RVs that are the number of successes in a set of trials, where each trial results in one of two outcomes: success or failure. We won’t go too much into detail on the math behind it, but here’s the formula for a binomial distribution:\n\\[\nP(k)=\\frac{n!}{k!(n-k!)}p^{k}(1-p)^{n-k}\n\\]\nwhere \\(k\\) is the number of successes, \\(n\\) is the number of trials, and \\(p\\) is the probability of success. \\(P(k)\\) is the probability of getting \\(k\\) successes.\n\n\nEvents\nSo now we know the probability distribution of John’s completions out of a set number of throws. If we had John actually go through with this experiment and we kept track of his throws and whether they were completed or not, we should get actual values to \\(X_{1}\\) and \\(X_{2}\\). When we collect a value for \\(X_{1}\\) one time, we’ll get either a completion or a turnover. This is known as an event and will be part of our sample (though right now it’s the only event in out sample). Graphing that, it won’t look like the probability distribution we laid out for \\(X_{1}\\). However, if we have John attempt this a large number of times, our results should resemble the probability distribution we assigned it. This describes the notion that the results of a sequence of RVs will converge towards their expectation as we gather more and more events in our sample, otherwise known as the law of large numbers. This theorem is one of the main principles of probability theory. Let’s test it out and run 10,000 events, or trials, for \\(X_{1}\\):\n\n\nCode\n```{python}\nn, p = 1, 0.5\ns = np.random.binomial(n, p, 10000)\ndf = pd.Series(s).value_counts() / len(s)\ndf = df.sort_index()\n\ncompletions = list(map(str, list(df.keys())))\nfreq = list(df.values)\n\nprint(df)\n\nfig = plt.figure(figsize = (6, 3))\nplt.bar(completions, freq, color = 'salmon', width = 0.4)\nplt.xlabel(\"Number of Completed Throws\")\nplt.ylabel(\"Relative Frequency\")\nplt.title(\"10,000 Events for $X_1$\")\nplt.show()\n```\n\n\n0    0.5041\n1    0.4959\ndtype: float64\n\n\n\n\n\nThat looks pretty close to me. Let’s test out \\(X_{2}\\) now:\n\n\nCode\n```{python}\nn, p = 2, 0.5\ns = np.random.binomial(n, p, 10000)\ndf = pd.Series(s).value_counts() / len(s)\ndf = df.sort_index()\n\ncompletions = list(map(str, list(df.keys())))\nfreq = list(df.values)\n\nprint(df)\n\nfig = plt.figure(figsize = (6, 3))\nplt.bar(completions, freq, color = 'salmon', width = 0.4)\nplt.xlabel(\"Number of Completed Throws\")\nplt.ylabel(\"Relative Frequency\")\nplt.title(\"10,000 Events for $X_2$\")\nplt.show()\n```\n\n\n0    0.2444\n1    0.5023\n2    0.2533\ndtype: float64\n\n\n\n\n\nLooks like it worked! Now that we know a little bit about random variables and their underlying probability distributions, we can further delve into the world of probabilities.\n\n\nProbability Theory\nProbability theory is, simply put, the math behind probabilities. We’ve already worked with the three main parts of probability theory:\n\nrandom variables\nprobability distributions\nevents\n\nBecause of this, we arguably just covered probability theory, but let’s go through some of the basics of how probabilities work just for good measure.\nProbability Principles:\n\n\\(P(x) \\in [0,1]\\)\n\\(P(\\Omega) = 1\\)\n\nLet’s start off with how probabilities work. The probability of \\(x\\), denoted by \\(P(x)\\), is the chance that \\(x\\) occurs. It must be a number between and including 0 and 1. A probability of 0 means the outcome will never occur, while a probability of 1 means the outcome will always occur. \\(\\Omega\\) denotes our sample space, or all the possible outcomes of the given event. The probability of one of these outcomes occurring is 1, as no other outcome can occur (otherwise it would be in the sample space).\nThe above was pretty intuitive, but it’s important to understand those concepts well. Now, let’s go over some useful definitions.\nProbability Definitions:\n\nIndependence: \\(P(A|B) = P(A)\\) or \\(P(B|A) = P(B)\\)\nMutually exclusive: \\(P(A\\cap B) = 0\\)\n\nTwo events are independent if the occurrence of one event does not change the probability of the other event. \\(P(A|B)\\) stands for the probability of \\(A\\) occurring given that \\(B\\) occurs. If that is equal to the probability of \\(A\\), then we can conclude that occurrence of \\(B\\) is independent of the occurrence of \\(A\\). The second definition describes two events that can’t both occur. One easy example is John throwing a completion and an incompletion. He can throw either one, but he can’t throw both at the same time. \\(P(A\\cap B)\\) stands for the probability of \\(A\\) and \\(B\\) both occurring. This is known as the intersection of \\(A\\) and \\(B\\). When that is equal to 0, \\(A\\) and \\(B\\) will never both occur, thus making them mutually exclusive. Another term for this is disjoint.\nThe above rules will be handy in the future, and we learned some important denotations along the way. Here’s two more: \\(P(A\\cup B\\) stands for the probability of \\(A\\) and/or \\(B\\) occurring. This is known as the union of \\(A\\) and \\(B\\). And \\(P(A')\\) stands for the probability of \\(A\\) not occurring. This is known as the complement of \\(A\\). Keeping these mind, let’s try some more complicated rules.\n\nAddition Rule: \\(P(A\\cup B) = P(A) + P(B) - P(A\\cap B)\\)\nMultiplication Rule: \\(P(A\\cup B) = P(A)\\times P(B|A) = P(B)\\times P(A|B)\\)\nComplement Rule: \\(P(A') = 1 - P(A)\\)\n\nThe addition rule describes how the union of two events works. Subtracting the intersection of both events is the key. It’s necessary because by adding the individual probabilities of the two events, you are inadvertently adding the intersection twice. Note that for mutually exclusive events \\(A\\) and \\(B\\), \\(P(A\\cap B) = 0\\), making \\(P(A\\cup B) = P(A) + P(B)\\). The multiplication rule further describes how union works. With this rule, if \\(A\\) and \\(B\\) are independent, then \\(P(A|B) = P(A)\\) and \\(P(B|A) = P(B)\\), making \\(P(A\\cup B) = P(A) * P(B)\\). The final rule describes how the complement of an event works. This can be reordered to show \\(P(A) + P(A') = 1\\), which makes intuitive sense. The probability of an event occurring plus the probability of that event not occurring should equal 1. The event has to either occur or not occur.\n\n\nMachine Learning Applications\nProbability theory is at the foundation of machine learning. ML algorithms typically make assumptions on the data using probability theory. These algorithms use those assumptions to make estimates and predictions on whatever unknown variable (like a random variable!) we’re focused on.\nLet’s go through an example. Say we are trying to estimate \\(Y\\), the number of throws that occur in a given possession for Burn. We want to know if \\(Z\\), the possession result (either a score or a turnover), is independent of \\(Y\\). First, let’s graph a distribution plot.\n\n\nCode\n```{python}\nimport seaborn as sns\npossession = dat.groupby('Full.ID') \\\n                .agg(Throws=('Possession.Result', 'size'), Score=('Possession.Result', 'mean')) \\\n                .reset_index()\nsns.displot(possession, x=\"Throws\", kde=True, hue=\"Score\", stat=\"count\")\nplt.show()\n```\n\n\n\n\n\nThe probability distribution of the number of throws appears to change significantly based on if the possession resulted in a score or not. Let’s try estimating this phenomenon with a ML algorithm.\n\n\nCode\n```{python}\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error                # metric for evaluating regression model\nfrom sklearn.metrics import mean_absolute_error               # metric for evaluating regression model\nfrom sklearn.metrics import r2_score                          # metric for evaluating regression model\n\nX = possession['Score']\nX = X.array.reshape(-1, 1)\ny = possession['Throws']\nlin_reg = LinearRegression().fit(X, y)\n\ny_pred = lin_reg.predict(X)\nprint(\"Linear Regression Results:\")\nprint(\"MAE:\", mean_absolute_error(y, y_pred))\nprint(\"MSE:\", mean_squared_error(y, y_pred))\nprint(\"r2:\", r2_score(y, y_pred))\n```\n\n\nLinear Regression Results:\nMAE: 3.2052691321520363\nMSE: 16.764008697346565\nr2: 0.053219132145364445\n\n\nWe just used the basics of probability theory to build a machine learning model. As you can tell by its low score (just a \\(r^{2}\\) value of 0.05), this model doesn’t run that well. Maybe that means that \\(Y\\) and \\(Z\\) are actually independent of each other (\\(P(Y|Z) = P(Y)\\)) and the differences we saw were just random error. Maybe the effect \\(Z\\) has on \\(Y\\) is just very slight.\n\n\nConclusion\nHopefully you now understand the basics of probability theory, the foundation for wondrous world of machine learning. Be sure to check out the next installment of this series, and have a great day!"
  },
  {
    "objectID": "blog/clustering/index.html",
    "href": "blog/clustering/index.html",
    "title": "Clustering",
    "section": "",
    "text": "This is the second blog post in a series of posts for my class on machine learning at Virginia Tech. Here we will discuss clustering, a common technique in machine learning that involves the grouping of data points together. We’ll cover the underlying objectives of clustering and dive into one of the most popular clustering methods: k-means.\n\nThe Data\nWe’ve already introduced the dataset we’ll be using for this blog series in the previous blog. In case you didn’t read that one, however, here’s a quick summary.\nThe data was collected by Burn, the VT Men’s Ultimate Club. Ultimate, also known simply as frisbee, is a game that involves players trying to throw a disc between themselves with the goal of being in their team’s endzone with the disc, scoring them a point. Don’t worry too much about the intricacies of ultimate; the machine learning concepts should still make sense regardless.\nI mentioned in my last blog that we’ll be using spinoffs of the initial dataset, which contained rows of throws from Burn’s offense at a recent tournament. That’s going to be the case in this blog. Burn has gone through the trouble of summarizing the data for each player on their team. We’ll be using this dataset for our clustering purposes.\n\n\nCode\n```{python}\nimport numpy as np\nimport pandas as pd\ndat = pd.read_csv(\"playDatSteelCity23.csv\")\ndat = dat.drop(['Unnamed: 0'], axis=1)\ndat.head()\n```\n\n\n\n\n\n\n\n\n\nPlayer\nActivePoss\nActivePossScored\nScorePerc\nTouches\nPickUp\nCenterThrow\nDumpThrow\nSwingThrow\nUpLineThrow\n...\nRecEPA\nHanEPARes\nHanEPA2\nHanEPATurn\nRecEPATurn\nEPA\nTourneyScore\nTourneyGrade\nEPAPerPoss\nEPAPerTouch\n\n\n\n\n0\nAR\n20\n7\n35.0\n54\n6\n0\n0\n9\n2\n...\n0.398\n0.718\n1.108\n-0.420\n0.000\n1.248\n1.00\nA\n0.062377\n0.023103\n\n\n1\nAK\n10\n5\n50.0\n25\n0\n0\n0\n2\n0\n...\n0.200\n0.300\n0.307\n0.000\n0.000\n0.248\n0.33\nB\n0.024833\n0.009933\n\n\n2\nJL\n41\n15\n36.6\n115\n2\n0\n3\n21\n6\n...\n1.843\n0.956\n-0.073\n-2.074\n-0.416\n1.512\n0.95\nB\n0.036869\n0.013145\n\n\n3\nZA\n16\n7\n43.8\n40\n4\n1\n1\n5\n3\n...\n-0.101\n0.415\n0.589\n-0.829\n-0.209\n0.230\n0.59\nB\n0.014361\n0.005744\n\n\n4\nCM\n29\n10\n34.5\n119\n19\n2\n5\n12\n5\n...\n0.229\n0.637\n0.612\n-1.915\n0.000\n0.583\n0.16\nC\n0.020089\n0.004896\n\n\n\n\n5 rows × 92 columns\n\n\n\n\n\nClustering\nClustering is a technique in machine learning that involves grouping similar data points together based on their inherent characteristics. It is an unsupervised learning method, meaning it does not require labeled data for training. This is because we aren’t necessary trying to estimate or predict anything directly from clustering. Instead, we aim to discover patterns or structures within the data, enabling us to gain insights. Thus, we don’t need our data to have labels, or context.\nLet’s consider a simple example. Say we have a group of different animal species and we want to create subgroups of the animals based on similarity. Some animals, like lions and tigers, are extremely similar and will likely get grouped together. Other animals, like whales and mice, are not similar at all and will likely not get grouped together. From these groupings, we can analyze for patterns. We might notice that fish are all grouped together, indicating they’re distinctly different type of animals then different animal classes. We might notice some exceptions, however, like dolphins (mammal) being grouped with sharks (fish). If this were a supervised learning method, we’d need information on the variables we’re trying to estimate. If we were trying to estimate an animal’s weight, for instance, we’d need data on animals and their weights to build the model in the first place. Since we’re not trying to estimate anything, however, we don’t absolutely need the animal’s weight to make these groups, though an important feature like that will certainly help.\nWhile that example didn’t entail math, clustering ML certainly does. Let’s introduce the problem we’ll be using for the rest of this blog. As I mentioned, the data we’ll be using contains 26 rows of players on Burn. We’ll be creating clusters of these players based on the number of times they pick up the disc (to start a possession) and their average throw distance. Let’s plot the players using those two metrics:\n\n\nCode\n```{python}\nimport matplotlib.pyplot as plt\nx1 = dat['PickUp']\nx2 = dat['AvgThrow']\n\nfig = plt.figure(figsize = (6, 3))\nax = fig.add_subplot(111)\nplt.scatter(x1, x2, color = 'tan', s = 50, alpha = 0.5)\nplt.xlabel(\"Pick Ups\")\nplt.ylabel(\"Average Throw Distance\")\nplt.title(\"Pick Ups versus Throw Distance\")\nplt.show()\n```\n\n\n\n\n\nJust from the eye test, you can likely see some clustering going on. The eye test is nice, but let’s try out a more objective method.\n\n\n\\(k\\)-Means\nOne commonly used clustering method is k-means clustering. In this algorithm, the data is divided into \\(k\\) clusters, where \\(k\\) is a parameter we define ourselves. The goal of the algorithm is to minimize the distance between data points and the center of the cluster they’re assigned to (referred to as centroids). Here is the general structure for the algorithm:\n\nInitialize: select \\(k\\) data points as the initial cluster centroids\nAssign: appoint each of our data points to the cluster whose centroid they’re closest to\nUpdate: recalculate the cluster centroids by taking the mean of all data points assigned to each cluster\nRepeat: complete steps 2 and 3 until convergence\n\nThe algorithm is essentially entails updating the cluster of each data point and the centroid of each cluster until updating no longer change the results significantly or we get tired of it taking too long! These steps seem simple enough, but carrying it out will require further clarification. For instance, how do we decide on the value of \\(k\\)? How do we decide which centroid each data point is closest to?\nLet’s start by answering the first question. Selecting \\(k\\), the number of clusters, is a little messy. Having to choose that value at the very beginning of the algorithm is one of its largest downsides. The goal is to find the optimal \\(k\\), one that minimizes the distance of data points within a cluster while maintaining the distance of between clusters. Choosing a too small \\(k\\) can result in forcing data points that aren’t close to be in the same cluster, while choosing a too large \\(k\\) can result in little insights gained, as clusters become more individualized to each data point. Ideally, we’d have domain knowledge that may indicate the optimal number of clusters. If we don’t, however, we’d likely have to turn to a selection technique, like the elbow method or silhouette analysis.\n\n\nElbow Method\nThe elbow method involves plotting the within-cluster sum of squares against different values of \\(k\\). The within-cluster sum of squares essentially represents the distance within the cluster between their data points, where smaller values mean the data points in the same clusters are closer together. We’d expect this value to decrease as we increase \\(k\\). After building this plot, we select the value where the decrease in the within-cluster sum of squares levels off. This is described as the “elbow” in the graph, as ideally the graph starts at a significant decline than suddenly flattens.\n\n\nCode\n```{python}\nfrom sklearn.cluster import KMeans\nX = np.array(list(zip(x1, x2))).reshape(len(x1), 2)\nK = range(1, 11)\nwcss = []\n\nfor k in K: \n    kmeans = KMeans(n_clusters = k, n_init = 10, random_state = 42)\n    kmeans.fit(X) \n    wcss.append(kmeans.inertia_)\n  \nplt.plot(K, wcss, 'bx-')\nplt.xlabel('$k$')\nplt.ylabel('Within-Cluster Sum of Squares')\nplt.title('Elbow Method')\nplt.show()\n```\n\n\n\n\n\nThe elbow here is at \\(k=2\\), so we’d choose 2 as our value for \\(k\\) when running \\(k\\)-Means. That’s just one method, however. Let’s try another one.\n\n\nSilhouette Analysis\nSilhouette analysis calculates a measure of how well each data point fits into its assigned cluster, helping to assess the quality of the clustering. It does this by calculating the average silhouette coefficient across all data points. The silhouette coefficient ranges from -1 to 1, where values close to 1 indicate the respective data point is in a suitable cluster and values close to -1 indicate the data point is in the wrong cluster. By calculating the average silhouette coefficient for different values of \\(k\\), we can identify the value that maximizes the overall closeness within clusters and distance between clusters.\nFrom using the elbow method, we can tell that the optimal value for \\(k\\) is likely 2. Let’s use silhouette analysis to compare \\(k=2\\) and \\(k=3\\) to get some more confirmation.\n\n\nCode\n```{python}\nimport matplotlib.cm as cm\nfrom sklearn.metrics import silhouette_samples, silhouette_score\n\nK = range(2, 4)\n\nfor k in K:\n    fig = plt.figure(figsize = (6, 3))\n    ax = fig.add_subplot(111)\n    ax.set_xlim([-0.1, 1])\n    ax.set_ylim([0, len(X) + (k + 1) * 10])\n    kmeans = KMeans(n_clusters = k, n_init = 10, random_state = 42)\n    labels = kmeans.fit_predict(X)\n    silhouette_avg = silhouette_score(X, labels)\n    sample = silhouette_samples(X, labels)\n\n    y_lower = 10\n    for i in range(k):\n        cluster_values = sample[labels == i]\n        cluster_values.sort()\n        size = cluster_values.shape[0]\n        y_upper = y_lower + size\n        color = cm.nipy_spectral(float(i) / k)\n        ax.fill_betweenx(\n            np.arange(y_lower, y_upper),\n            0,\n            cluster_values,\n            facecolor=color,\n            edgecolor=color,\n            alpha=0.7,\n        )\n        ax.text(-0.05, y_lower + 0.5 * size, str(i))\n        y_lower = y_upper + 10\n\n    ax.set_title(\"Silhouette analysis ($k = %d$)\" % k)\n    ax.set_xlabel(\"Silhouette Coefficient\")\n    ax.set_ylabel(\"Cluster\")\n    ax.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n    ax.set_yticks([])\n    ax.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n\nplt.show()\n```\n\n\n\n\n\n\n\n\nThe results of the silhouette analysis confirm what we found with the elbow method. We’ll use \\(k=2\\).\n\n\nResults\nNow that we have our value for \\(k\\), let’s flesh out the rest of the \\(k\\)-Means algorithm. We’ll randomly select our initial centroids. This is a limitation of \\(k\\)-Means, as we could converge to a locally optimal result. Without any domain knowledge to guide us, however, it’s our easiest approach. We’ll also use Euclidean distance to define how close each data point is to the centroids. This is one of the more commonly used distance metrics.\nWith that, we should be ready to run \\(k\\)-Means.\n\n\nCode\n```{python}\nk = 2\nfig = plt.figure(figsize = (6, 3))\nax = fig.add_subplot(111)\nkmeans = KMeans(n_clusters = k, n_init = 10, random_state = 42)\nkmeans.fit(X) \nlabels = kmeans.fit_predict(X)\ncolors = cm.nipy_spectral(labels.astype(float) / k)\nax.scatter(X[:, 0], X[:, 1], s = 50, alpha = 0.5, c = colors)\nax.set_xlabel(\"Pick Ups\")\nax.set_ylabel(\"Average Throw Distance\")\nax.set_title(\"Pick Ups versus Throw Distance\")\nplt.show()\n```\n\n\n\n\n\nIt appears as though the separator between the two clusters is the number of times the players picked up the disc. In ultimate, there’s a set role for the players that pick up the disc: the handler. The machine learning we did helped us see the number of handlers on the team and, when we check their names, who the handlers are. Great work!\n\n\nConclusion\nHopefully you now understand the basics of clustering. Be sure to check out the next installment of this series, and have a great day!"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "My name is Spencer Paragas. I am an associate analyst at Altria Client Services, as well as a graduate student in the Computer Science Masters of Engineering program at Virginia Tech. This blog is currently set up as the final project in my graduate class on machine learning. My hope, however, is that this blog will eventually turn into something beyond just this class.\nNotes on this blog:\nPrefers github pages, netlify is allowed but consider switching (he has a how-to on his website)\nAdd a “view source” link to every page on the website (apparently easily down thru quarto)\n#| warning: false\n#| echo: fenced (shows whole code chunk including the language you’re using)\nCan put this under execute in quarto.yml so it works for all code chunks\nHis quarto blog post is considered too much text (should be half), too much scrolling (don’t make read time too long, short and snappy)\nSubmit the website early and he’ll try to give his idea on what grade you’ll get. For feedback, go to TA office hours"
  },
  {
    "objectID": "blog/blog.html",
    "href": "blog/blog.html",
    "title": "CS 5805 Final Project",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "",
    "section": "",
    "text": "Code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProbability Theory and Random Variables\n\n\n\ncs5805final\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\nSpencer Paragas\n\n\nDec 6, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nClustering\n\n\n\ncs5805final\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\nSpencer Paragas\n\n\nDec 6, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/probability/index.html#probability-distributions",
    "href": "blog/probability/index.html#probability-distributions",
    "title": "Probability Theory and Random Variables",
    "section": "Probability Distributions",
    "text": "Probability Distributions\nLet’s look at an example in the context of our ultimate data. Say we know that on any given throw, John has a 50% chance of completing the throw without turning it over to the other team. Our random variable \\(X_{1}\\) is the number of completed throws John makes off of 1 throw. What would the resulting probability distribution look like? This one’s pretty simple, he has a 50% chance of completing the one throw, and he has a 50% chance of not completing it. Thus, the probability distribution looks like this:\n\nCode```{python}\nimport matplotlib.pyplot as plt\nx = {'0':0.5, '1':0.5}\nprobabilities = list(x.keys())\ncompletions = list(x.values())\n\nfig = plt.figure(figsize = (10, 5))\nplt.bar(probabilities, completions, color = 'blue', width = 0.4)\nplt.xlabel(\"Number of Completed Throws\")\nplt.ylabel(\"Probability\")\nplt.show()\n```\n\n&lt;BarContainer object of 2 artists&gt;\n\n\n\n\n\n\nCode```{r}\n# What is probability theory?\nprint(\"Hello World\")\n```\n\n[1] \"Hello World\""
  }
]