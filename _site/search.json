[
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Clustering",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\n# What are some clustering methods?"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About",
    "section": "",
    "text": "My name is Spencer Paragas. I am an associate analyst at Altria Client Services, as well as a graduate student in the Computer Science Masters of Engineering program at Virginia Tech. This blog is currently set up as the final project in my graduate class on machine learning. My hope, however, is that this blog will eventually turn into something beyond just this class.\nNotes on this blog:\nPrefers github pages, netlify is allowed but consider switching (he has a how-to on his website)\nAdd a “view source” link to every page on the website (apparently easily down thru quarto)\n#| warning: false\n#| echo: fenced (shows whole code chunk including the language you’re using)\nCan put this under execute in quarto.yml so it works for all code chunks\nHis quarto blog post is considered too much text (should be half), too much scrolling (don’t make read time too long, short and snappy)\nSubmit the website early and he’ll try to give his idea on what grade you’ll get. For feedback, go to TA office hours"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "CS 5805 Final Project",
    "section": "",
    "text": "Probability Theory and Random Variables\n\n\n\n\n\n\n\ncs5805final\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 30, 2023\n\n\nSpencer Paragas\n\n\n\n\n\n\n  \n\n\n\n\nClustering\n\n\n\n\n\n\n\ncs5805final\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 30, 2023\n\n\nSpencer Paragas\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Probability Theory and Random Variables",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2\n\n\n\n# What is probability theory?\nprint(\"Hello World\")\n\n[1] \"Hello World\""
  },
  {
    "objectID": "blog/welcome/index.html",
    "href": "blog/welcome/index.html",
    "title": "Clustering",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\n\nCode\n```{python}\n# What are some clustering methods?\n2+2\n```\n\n\n4"
  },
  {
    "objectID": "blog/post-with-code/index.html",
    "href": "blog/post-with-code/index.html",
    "title": "Probability Theory and Random Variables",
    "section": "",
    "text": "This is a post with executable code.\n\nCode```{r}\n1 + 1\n```\n\n[1] 2\n\n\n\nCode```{r}\n# What is probability theory?\nprint(\"Hello World\")\n```\n\n[1] \"Hello World\""
  },
  {
    "objectID": "blog/probability theory/index.html",
    "href": "blog/probability theory/index.html",
    "title": "Probability Theory and Random Variables",
    "section": "",
    "text": "This is a post with executable code.\n\nCode```{r}\n1 + 1\n```\n\n[1] 2\n\n\n\nCode```{r}\n# What is probability theory?\nprint(\"Hello World\")\n```\n\n[1] \"Hello World\""
  },
  {
    "objectID": "blog/welcome/cluster.html",
    "href": "blog/welcome/cluster.html",
    "title": "Clustering",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\n\nCode\n```{python}\n# What are some clustering methods?\n2+2\n```\n\n\n4"
  },
  {
    "objectID": "blog/probability/index.html",
    "href": "blog/probability/index.html",
    "title": "Probability Theory and Random Variables",
    "section": "",
    "text": "This is the first blog post in a series of posts for my class on machine learning at Virginia Tech. Here we will discuss probability theory, which is the foundation to the later machine learning posts in this series. While probability theory is a massive branch of mathematics that can’t be fully covered in one blog, we’ll go over topics like random variables, conditional probability, and independence.\n\nThe Data\nBefore that, however, let’s introduce the dataset we’ll be using for the entirety of this blog series. The data was collected by Burn, the VT Men’s Ultimate Club. Ultimate, also known simply as frisbee, is a game that involves players trying to throw a disc between themselves with the goal of being in their team’s endzone with the disc, scoring them a point. For many of you, this game may be foreign, making some of the variables confusing. While you don’t need to know most of the context to understand the machine learning concepts shown, I’ll do my best to explain the important parts as succinctly as possible.\nThe data represents the performance of the ultimate club at Steel City, a tournament in Pittsburgh they went to this Fall. Only data on Burn’s offense (when they had the disc) was collected. In the initial dataset, each row represents an action with the disc, either throwing it or picking it up. The features include variables like the opponents they were facing, who threw it to who, the type of throw it was, and the outcome of the throw. In total the dataset is \\(986 \\times 40\\), though spinoffs of the dataset will be used later on in the series.\n\n\nCode\n```{python}\nimport numpy as np\nimport pandas as pd\ndat = pd.read_csv(\"currDatSteelCity23.csv\")\ndat = dat.drop(['Unnamed: 0'], axis=1)\ndat.head()\n```\n\n\n\n\n\n\n\n\n\nOpponent\nidx\nDJI.ID\nFull.ID\nGame.ID\nPoint.ID\nPossession.ID\nDefense.Type\nHandler\nReceiver\n...\nBasicEPAOpp\nTrueEPAHan\nTrueEPARec\nEPA\nOppFactor2\nEPAHan\nEPARec\nEPAHanRes\nEPAHanTot\nEPAHan2\n\n\n\n\n0\nBinghamton\n538\n472\n1.1.1\n1\n1\n1\nMan\nNaN\nMicah\n...\n0.000\n0.000\n0.000\n0.000\n0.0\nNaN\nNaN\n0.000\nNaN\nNaN\n\n\n1\nBinghamton\n539\n472\n1.1.1\n1\n1\n1\nMan\nMicah\nJohn\n...\n0.018\n0.014\n0.004\n0.053\n0.0\n0.039\n0.014\n0.015\n0.054\n0.059\n\n\n2\nBinghamton\n540\n472\n1.1.1\n1\n1\n1\nMan\nJohn\nDan B\n...\n0.011\n0.008\n0.002\n0.042\n0.0\n0.031\n0.011\n0.000\n0.031\n0.046\n\n\n3\nBinghamton\n541\n472\n1.1.1\n1\n1\n1\nMan\nDan B\nGribs\n...\n-0.249\n-0.249\n0.000\n-0.421\n0.0\n-0.421\n0.000\n0.000\n-0.421\n-0.421\n\n\n4\nBinghamton\n542\n472\n1.1.2\n1\n1\n2\nMan\nNaN\nMicah\n...\n0.000\n0.000\n0.000\n0.000\n0.0\nNaN\n0.000\n0.000\nNaN\nNaN\n\n\n\n\n5 rows × 40 columns\n\n\n\n\n\nRandom Variables\nA random variable (RV) is a variable of unknown value that represents the numerical outcome of some process. This is vague, but essentially an RV can’t be known (hence it’s a variable), has to be numerical, and has to have some kind of context to it. RVs are typically represented by capitalized letters, like \\(X\\) or \\(Y\\). Some examples include the number of coins that flip to head out of 2 or the height of a person. The former example is one where the RV is discrete: it can only be a distinct value like 0, 1, or 2. The latter example is one where the RV is continuous: it can be any of the infinite values within a range, like any number between 0 and 1000.\nThe significance of random variables in the world of probability theory and, more broadly, statistics in general is that RVs have probability distributions that allow us to calculate the probabilities of RVs. This process has many applications, where people try to understand and estimate real world RVs.\n\n\nProbability Distributions\nLet’s look at an example in the context of our ultimate data. Say we know that on any given throw, John has a 50% chance of completing the throw without turning it over to the other team. Our random variable \\(X_{1}\\) is the number of completed throws John makes off of 1 throw. This is RV is discrete. What would the resulting probability distribution look like? This one’s pretty simple, he has a 50% chance of completing the one throw, and he has a 50% chance of not completing it. Thus, the probability distribution looks like this:\n\n\nCode\n```{python}\nimport matplotlib.pyplot as plt\nx = {'0':0.5, '1':0.5}\ncompletions = list(x.keys())\nprobabilities = list(x.values())\n\nfig = plt.figure(figsize = (6, 3))\nplt.bar(completions, probabilities, color = 'cornflowerblue', width = 0.4)\nplt.xlabel(\"Number of Completed Throws\")\nplt.ylabel(\"Probability\")\nplt.title(\"Probability Distribution for $X_1$\")\nplt.show()\n```\n\n\n\n\n\nWhat about the probability distribution of the random variable \\(X_{2}\\), the number of completed throws John makes off of 2 throws? Each throw has an equal chance of being a completion or an incompletion (50%). Half of the time John will complete the first throw, and half of those times John will complete the second throw. That gives the probability of John completing 2 out of 2 throws 0.25. The same logic can give you the probability of 0 completions. Thus, the probability distribution for \\(X_{2}\\) looks like this:\n\n\nCode\n```{python}\nx = {'0':0.25, '1':0.5, '2':0.25}\ncompletions = list(x.keys())\nprobabilities = list(x.values())\n\nfig = plt.figure(figsize = (6, 3))\nplt.bar(completions, probabilities, color = 'cornflowerblue', width = 0.4)\nplt.xlabel(\"Number of Completed Throws\")\nplt.ylabel(\"Probability\")\nplt.title(\"Probability Distribution for $X_2$\")\nplt.show()\n```\n\n\n\n\n\n\\(X_{1}\\) and \\(X_{2}\\) are examples of random variables with a binomial distribution. This distribution represent RVs that are the number of successes in a set of trials, where each trial results in one of two outcomes: success or failure. We won’t go too much into detail on the math behind it, but here’s the formula for a binomial distribution:\n\\[\nP(k)=\\frac{n!}{k!(n-k!)}p^{k}(1-p)^{n-k}\n\\]\nwhere \\(k\\) is the number of successes, \\(n\\) is the number of trials, and \\(p\\) is the probability of success. \\(P(k)\\) is the probability of getting \\(k\\) successes.\n\n\nEvents\nSo now we know the probability distribution of John’s completions out of a set number of throws. If we had John actually go through with this experiment and we kept track of his throws and whether they were completed or not, we should get actual values to \\(X_{1}\\) and \\(X_{2}\\). When we collect a value for \\(X_{1}\\) one time, we’ll get either a completion or a turnover. This is known as an event and will be part of our sample (though right now it’s the only event in out sample). Graphing that, it won’t look like the probability distribution we laid out for \\(X_{1}\\). However, if we have John attempt this a large number of times, our results should resemble the probability distribution we assigned it. This describes the notion that the results of a sequence of RVs will converge towards their expectation as we gather more and more events in our sample, otherwise known as the law of large numbers. This theorem is one of the main principles of probability theory. Let’s test it out and run 10,000 events, or trials, for \\(X_{1}\\):\n\n\nCode\n```{python}\nn, p = 1, 0.5\ns = np.random.binomial(n, p, 10000)\ndf = pd.Series(s).value_counts() / len(s)\ndf = df.sort_index()\n\ncompletions = list(map(str, list(df.keys())))\nfreq = list(df.values)\n\nprint(df)\n\nfig = plt.figure(figsize = (6, 3))\nplt.bar(completions, freq, color = 'salmon', width = 0.4)\nplt.xlabel(\"Number of Completed Throws\")\nplt.ylabel(\"Relative Frequency\")\nplt.title(\"10,000 Events for $X_1$\")\nplt.show()\n```\n\n\n0    0.4922\n1    0.5078\ndtype: float64\n\n\n\n\n\nThat looks pretty close to me. Let’s test out \\(X_{2}\\) now:\n\n\nCode\n```{python}\nn, p = 2, 0.5\ns = np.random.binomial(n, p, 10000)\ndf = pd.Series(s).value_counts() / len(s)\ndf = df.sort_index()\n\ncompletions = list(map(str, list(df.keys())))\nfreq = list(df.values)\n\nprint(df)\n\nfig = plt.figure(figsize = (6, 3))\nplt.bar(completions, freq, color = 'salmon', width = 0.4)\nplt.xlabel(\"Number of Completed Throws\")\nplt.ylabel(\"Relative Frequency\")\nplt.title(\"10,000 Events for $X_2$\")\nplt.show()\n```\n\n\n0    0.2535\n1    0.4968\n2    0.2497\ndtype: float64\n\n\n\n\n\nLooks like it worked! Now that we know a little bit about random variables and their underlying probability distributions, we can further delve into the world of probabilities.\n\n\nProbability Theory\nProbability theory is, simply put, the math behind probabilities. We’ve already worked with the three main parts of probability theory:\n\nrandom variables\nprobability distributions\nevents\n\nBecause of this, we arguably just covered probability theory, but let’s go through some of the basics of how probabilities work just for good measure.\nProbability Principles:\n\n\\(P(x) \\in [0,1]\\)\n\\(P(\\Omega) = 1\\)\n\nLet’s start off with how probabilities work. The probability of \\(x\\), denoted by \\(P(x)\\), is the chance that \\(x\\) occurs. It must be a number between and including 0 and 1. A probability of 0 means the outcome will never occur, while a probability of 1 means the outcome will always occur. \\(\\Omega\\) denotes our sample space, or all the possible outcomes of the given event. The probability of one of these outcomes occurring is 1, as no other outcome can occur (otherwise it would be in the sample space).\nThe above was pretty intuitive, but it’s important to understand those concepts well. Now, let’s go over some useful definitions.\nProbability Definitions:\n\nIndependence: \\(P(A|B) = P(A)\\) or \\(P(B|A) = P(B)\\)\nMutually exclusive: \\(P(A\\cap B) = 0\\)\n\nTwo events are independent if the occurrence of one event does not change the probability of the other event. \\(P(A|B)\\) stands for the probability of \\(A\\) occurring given that \\(B\\) occurs. If that is equal to the probability of \\(A\\), then we can conclude that occurrence of \\(B\\) is independent of the occurrence of \\(A\\). The second definition describes two events that can’t both occur. One easy example is John throwing a completion and an incompletion. He can throw either one, but he can’t throw both at the same time. \\(P(A\\cap B)\\) stands for the probability of \\(A\\) and \\(B\\) both occurring. This is known as the intersection of \\(A\\) and \\(B\\). When that is equal to 0, \\(A\\) and \\(B\\) will never both occur, thus making them mutually exclusive. Another term for this is disjoint.\nThe above rules will be handy in the future, and we learned some important denotations along the way. Here’s two more: \\(P(A\\cup B\\) stands for the probability of \\(A\\) and/or \\(B\\) occurring. This is known as the union of \\(A\\) and \\(B\\). And \\(P(A')\\) stands for the probability of \\(A\\) not occurring. This is known as the complement of \\(A\\). Keeping these mind, let’s try some more complicated rules.\n\nAddition Rule: \\(P(A\\cup B) = P(A) + P(B) - P(A\\cap B)\\)\nMultiplication Rule: \\(P(A\\cup B) = P(A)\\times P(B|A) = P(B)\\times P(A|B)\\)\nComplement Rule: \\(P(A') = 1 - P(A)\\)\n\nThe addition rule describes how the union of two events works. Subtracting the intersection of both events is the key. It’s necessary because by adding the individual probabilities of the two events, you are inadvertently adding the intersection twice. Note that for mutually exclusive events \\(A\\) and \\(B\\), \\(P(A\\cap B) = 0\\), making \\(P(A\\cup B) = P(A) + P(B)\\). The multiplication rule further describes how union works. With this rule, if \\(A\\) and \\(B\\) are independent, then \\(P(A|B) = P(A)\\) and \\(P(B|A) = P(B)\\), making \\(P(A\\cup B) = P(A) * P(B)\\). The final rule describes how the complement of an event works. This can be reordered to show \\(P(A) + P(A') = 1\\), which makes intuitive sense. The probability of an event occurring plus the probability of that event not occurring should equal 1. The event has to either occur or not occur.\n\n\nConclusion\nHopefully you now understand the basics of probability theory, the foundation for wondrous world of machine learning. Be sure to check out the next installment of this series, and have a great day!"
  },
  {
    "objectID": "blog/clustering/index.html",
    "href": "blog/clustering/index.html",
    "title": "Clustering",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\n\nCode\n```{python}\n# What are some clustering methods?\n2+2\n```\n\n\n4"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "My name is Spencer Paragas. I am an associate analyst at Altria Client Services, as well as a graduate student in the Computer Science Masters of Engineering program at Virginia Tech. This blog is currently set up as the final project in my graduate class on machine learning. My hope, however, is that this blog will eventually turn into something beyond just this class.\nNotes on this blog:\nPrefers github pages, netlify is allowed but consider switching (he has a how-to on his website)\nAdd a “view source” link to every page on the website (apparently easily down thru quarto)\n#| warning: false\n#| echo: fenced (shows whole code chunk including the language you’re using)\nCan put this under execute in quarto.yml so it works for all code chunks\nHis quarto blog post is considered too much text (should be half), too much scrolling (don’t make read time too long, short and snappy)\nSubmit the website early and he’ll try to give his idea on what grade you’ll get. For feedback, go to TA office hours"
  },
  {
    "objectID": "blog/blog.html",
    "href": "blog/blog.html",
    "title": "CS 5805 Final Project",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "CS 5805 Final Project",
    "section": "",
    "text": "Probability Theory and Random Variables\n\n\n\ncs5805final\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\nSpencer Paragas\n\n\nDec 6, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nClustering\n\n\n\ncs5805final\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\nSpencer Paragas\n\n\nNov 30, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/probability/index.html#probability-distributions",
    "href": "blog/probability/index.html#probability-distributions",
    "title": "Probability Theory and Random Variables",
    "section": "Probability Distributions",
    "text": "Probability Distributions\nLet’s look at an example in the context of our ultimate data. Say we know that on any given throw, John has a 50% chance of completing the throw without turning it over to the other team. Our random variable \\(X_{1}\\) is the number of completed throws John makes off of 1 throw. What would the resulting probability distribution look like? This one’s pretty simple, he has a 50% chance of completing the one throw, and he has a 50% chance of not completing it. Thus, the probability distribution looks like this:\n\nCode```{python}\nimport matplotlib.pyplot as plt\nx = {'0':0.5, '1':0.5}\nprobabilities = list(x.keys())\ncompletions = list(x.values())\n\nfig = plt.figure(figsize = (10, 5))\nplt.bar(probabilities, completions, color = 'blue', width = 0.4)\nplt.xlabel(\"Number of Completed Throws\")\nplt.ylabel(\"Probability\")\nplt.show()\n```\n\n&lt;BarContainer object of 2 artists&gt;\n\n\n\n\n\n\nCode```{r}\n# What is probability theory?\nprint(\"Hello World\")\n```\n\n[1] \"Hello World\""
  }
]